{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUBRIC\n",
    "\n",
    "[2 Points] Present an overview for what type of bias you will be investigating and why the particular investigation you will be doing is relevant. You might consider asking questions like: Why is it important to find this kind of bias in machine learning models? Why will the type of investigation I am performing be relevant to other researchers or practitioners? \n",
    "\n",
    "[2 Points] Present one or more research questions that you will be answering and explain the methods that you will employ to answer these research questions. Present a hypothesis as part of your research questions. \n",
    "\n",
    "[2 Points] As part of your assignment, you will choose a methodology that involves comparing two (or more) techniques to one another. Discuss how you will measure a difference between the two techniques. That is, if you are measuring the difference statistically, what test will you use and why is it appropriate? Are there any limitations to performing this test that you should be aware of? \n",
    "\n",
    "[4 Points] Carryout your analysis and model training. Explain your steps in as much detail so that the instructor can understand your code. \n",
    "\n",
    "[4 Points] Present results from your analysis and provide evidence from the results that support or refute your hypothesis. Write a conclusion based upon the various analyses you performed. Be sure to reference your research questions systematically in your conclusion. With your analysis complete, are there any additional research questions or limitations to your conclusions?\n",
    "\n",
    "[1 Points] Identify two conferences or journals that would be interested in the results of your analysis.  \n",
    "If using code from another author (not your own), you will be graded on the clarity of explanatory comments you add to the code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5822/2903047699.py:5: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "/home/paperspace/Desktop/8321-Mach-Lrng-Neural-Ntwrks/remote-venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "from types import FunctionType\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchtext\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# BASE_PATH: str = os.path.dirname(os.path.abspath(__file__))\n",
    "BASE_PATH: str = \"/home/paperspace/Desktop/8321-Mach-Lrng-Neural-Ntwrks/Lab1/CS8321_Lab1\" # REPLACE THIS LINE FOR YOUR LOCAL\n",
    "\n",
    "CLASSIFIERS_PATH: str = BASE_PATH + \"/classifiers/\"\n",
    "DATASET_PATH: str = BASE_PATH + \"/datasets/\"\n",
    "EMBEDDINGS_PATH: str = BASE_PATH + \"/embeddings/\"\n",
    "NUM_EMOTIONS: int = 28\n",
    "EMBED_SIZE: int = 0\n",
    "\n",
    "# Check if our key directories exist\n",
    "if not os.path.exists(CLASSIFIERS_PATH):\n",
    "    raise FileNotFoundError(\"Could not find folder for classifier models.\")\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    raise FileNotFoundError(\"Could not find folder with GoEmotion dataset.\")\n",
    "if not os.path.exists(EMBEDDINGS_PATH):\n",
    "    raise FileNotFoundError(\"Could not find folder with word embeddings sets.\")\n",
    "\n",
    "# Is the cuda GPU available?\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: Using CPU for Pytorch.\")\n",
    "device: device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's define our basic BDRNN architecture\n",
    "class BDRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_word_count: int, vectors: torch.Tensor, output_size: int, num_layers: int, dropout: float,\n",
    "                 *args: tuple[any],\n",
    "                 **kwargs: dict[str, any]) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.num_layers = num_layers if num_layers > 1 else 2\n",
    "        self.hidden_size = NUM_EMOTIONS // num_layers\n",
    "\n",
    "        self.embeddings = torch.nn.Embedding.from_pretrained(vectors, padding_idx=EMBED_SIZE)\n",
    "\n",
    "        self.rnn_layers = torch.nn.RNN(input_size=vocab_word_count, hidden_size=self.hidden_size, num_layers=num_layers,\n",
    "                                       bidirectional=True, dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.output_layer = torch.nn.Linear(self.hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_data) -> torch.Tensor:\n",
    "        embedded: torch.Tensor = self.embeddings(input_data)\n",
    "\n",
    "        output: torch.Tensor\n",
    "        hidden: torch.Tensor\n",
    "        output, hidden = self.rnn_layers(embedded)\n",
    "\n",
    "        return self.output_layer(hidden[-1, :])\n",
    "\n",
    "\n",
    "class pandas_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame) -> None:\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index: int) -> (str, str):\n",
    "        return self.df[\"text\"].iloc[index], self.df[\"emotion_ids\"].iloc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100000/3000000\n",
      "Processed 200000/3000000\n",
      "Processed 300000/3000000\n",
      "Processed 400000/3000000\n",
      "Processed 500000/3000000\n",
      "Processed 600000/3000000\n",
      "Processed 700000/3000000\n",
      "Processed 800000/3000000\n",
      "Processed 900000/3000000\n",
      "Processed 1000000/3000000\n",
      "Processed 1100000/3000000\n",
      "Processed 1200000/3000000\n",
      "Processed 1300000/3000000\n",
      "Processed 1400000/3000000\n",
      "Processed 1500000/3000000\n",
      "Processed 1600000/3000000\n",
      "Processed 1700000/3000000\n",
      "Processed 1800000/3000000\n",
      "Processed 1900000/3000000\n",
      "Processed 2000000/3000000\n",
      "Processed 2100000/3000000\n",
      "Processed 2200000/3000000\n",
      "Processed 2300000/3000000\n",
      "Processed 2400000/3000000\n",
      "Processed 2500000/3000000\n",
      "Processed 2600000/3000000\n",
      "Processed 2700000/3000000\n",
      "Processed 2800000/3000000\n",
      "Processed 2900000/3000000\n",
      "Processed 3000000/3000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'</s>': 0,\n",
       "  'in': 1,\n",
       "  'for': 2,\n",
       "  'that': 3,\n",
       "  'is': 4,\n",
       "  'on': 5,\n",
       "  '##': 6,\n",
       "  'The': 7,\n",
       "  'with': 8,\n",
       "  'said': 9,\n",
       "  'was': 10,\n",
       "  'the': 11,\n",
       "  'at': 12,\n",
       "  'not': 13,\n",
       "  'as': 14,\n",
       "  'it': 15,\n",
       "  'be': 16,\n",
       "  'from': 17,\n",
       "  'by': 18,\n",
       "  'are': 19,\n",
       "  'I': 20,\n",
       "  'have': 21,\n",
       "  'he': 22,\n",
       "  'will': 23,\n",
       "  'has': 24,\n",
       "  '####': 25,\n",
       "  'his': 26,\n",
       "  'an': 27,\n",
       "  'this': 28,\n",
       "  'or': 29,\n",
       "  'their': 30,\n",
       "  'who': 31,\n",
       "  'they': 32,\n",
       "  'but': 33,\n",
       "  '$': 34,\n",
       "  'had': 35,\n",
       "  'year': 36,\n",
       "  'were': 37,\n",
       "  'we': 38,\n",
       "  'more': 39,\n",
       "  '###': 40,\n",
       "  'up': 41,\n",
       "  'been': 42,\n",
       "  'you': 43,\n",
       "  'its': 44,\n",
       "  'one': 45,\n",
       "  'about': 46,\n",
       "  'would': 47,\n",
       "  'which': 48,\n",
       "  'out': 49,\n",
       "  'can': 50,\n",
       "  'It': 51,\n",
       "  'all': 52,\n",
       "  'also': 53,\n",
       "  'two': 54,\n",
       "  'after': 55,\n",
       "  'first': 56,\n",
       "  'He': 57,\n",
       "  'do': 58,\n",
       "  'time': 59,\n",
       "  'than': 60,\n",
       "  'when': 61,\n",
       "  'We': 62,\n",
       "  'over': 63,\n",
       "  'last': 64,\n",
       "  'new': 65,\n",
       "  'other': 66,\n",
       "  'her': 67,\n",
       "  'people': 68,\n",
       "  'into': 69,\n",
       "  'In': 70,\n",
       "  'our': 71,\n",
       "  'there': 72,\n",
       "  'A': 73,\n",
       "  'she': 74,\n",
       "  'could': 75,\n",
       "  'just': 76,\n",
       "  'years': 77,\n",
       "  'some': 78,\n",
       "  'U.S.': 79,\n",
       "  'three': 80,\n",
       "  'million': 81,\n",
       "  'them': 82,\n",
       "  'what': 83,\n",
       "  'But': 84,\n",
       "  'so': 85,\n",
       "  'no': 86,\n",
       "  'like': 87,\n",
       "  'if': 88,\n",
       "  'only': 89,\n",
       "  'percent': 90,\n",
       "  'get': 91,\n",
       "  'did': 92,\n",
       "  'him': 93,\n",
       "  'game': 94,\n",
       "  'back': 95,\n",
       "  'because': 96,\n",
       "  'now': 97,\n",
       "  '#.#': 98,\n",
       "  'before': 99,\n",
       "  'company': 100,\n",
       "  'any': 101,\n",
       "  'team': 102,\n",
       "  'against': 103,\n",
       "  'off': 104,\n",
       "  'This': 105,\n",
       "  'most': 106,\n",
       "  'made': 107,\n",
       "  'through': 108,\n",
       "  'make': 109,\n",
       "  'second': 110,\n",
       "  'state': 111,\n",
       "  'well': 112,\n",
       "  'day': 113,\n",
       "  'season': 114,\n",
       "  'says': 115,\n",
       "  'week': 116,\n",
       "  'where': 117,\n",
       "  'while': 118,\n",
       "  'down': 119,\n",
       "  'being': 120,\n",
       "  'government': 121,\n",
       "  'your': 122,\n",
       "  '#-#': 123,\n",
       "  'home': 124,\n",
       "  'going': 125,\n",
       "  'my': 126,\n",
       "  'good': 127,\n",
       "  'They': 128,\n",
       "  \"'re\": 129,\n",
       "  'should': 130,\n",
       "  'many': 131,\n",
       "  'way': 132,\n",
       "  'those': 133,\n",
       "  'four': 134,\n",
       "  'during': 135,\n",
       "  'such': 136,\n",
       "  'may': 137,\n",
       "  'very': 138,\n",
       "  'how': 139,\n",
       "  'since': 140,\n",
       "  'work': 141,\n",
       "  'take': 142,\n",
       "  'including': 143,\n",
       "  'high': 144,\n",
       "  'then': 145,\n",
       "  '%': 146,\n",
       "  'next': 147,\n",
       "  '#,###': 148,\n",
       "  'By': 149,\n",
       "  'much': 150,\n",
       "  'still': 151,\n",
       "  'go': 152,\n",
       "  'think': 153,\n",
       "  'old': 154,\n",
       "  'even': 155,\n",
       "  '#.##': 156,\n",
       "  'world': 157,\n",
       "  'see': 158,\n",
       "  'say': 159,\n",
       "  'business': 160,\n",
       "  'five': 161,\n",
       "  'told': 162,\n",
       "  'under': 163,\n",
       "  'us': 164,\n",
       "  '1': 165,\n",
       "  'these': 166,\n",
       "  'If': 167,\n",
       "  'right': 168,\n",
       "  'And': 169,\n",
       "  'me': 170,\n",
       "  'between': 171,\n",
       "  'play': 172,\n",
       "  'help': 173,\n",
       "  '##,###': 174,\n",
       "  'market': 175,\n",
       "  'That': 176,\n",
       "  'know': 177,\n",
       "  'end': 178,\n",
       "  'AP': 179,\n",
       "  'long': 180,\n",
       "  'information': 181,\n",
       "  'points': 182,\n",
       "  'does': 183,\n",
       "  'both': 184,\n",
       "  'There': 185,\n",
       "  'part': 186,\n",
       "  'around': 187,\n",
       "  'police': 188,\n",
       "  'want': 189,\n",
       "  \"'ve\": 190,\n",
       "  'based': 191,\n",
       "  'For': 192,\n",
       "  'got': 193,\n",
       "  'third': 194,\n",
       "  'school': 195,\n",
       "  'left': 196,\n",
       "  'another': 197,\n",
       "  'country': 198,\n",
       "  'need': 199,\n",
       "  '2': 200,\n",
       "  'best': 201,\n",
       "  'win': 202,\n",
       "  'quarter': 203,\n",
       "  'use': 204,\n",
       "  'today': 205,\n",
       "  '##.#': 206,\n",
       "  'same': 207,\n",
       "  'public': 208,\n",
       "  'run': 209,\n",
       "  'Friday': 210,\n",
       "  'set': 211,\n",
       "  'month': 212,\n",
       "  'top': 213,\n",
       "  'billion': 214,\n",
       "  'Tuesday': 215,\n",
       "  'come': 216,\n",
       "  'Monday': 217,\n",
       "  'She': 218,\n",
       "  'city': 219,\n",
       "  'place': 220,\n",
       "  'night': 221,\n",
       "  'six': 222,\n",
       "  'each': 223,\n",
       "  'Thursday': 224,\n",
       "  '###,###': 225,\n",
       "  'Wednesday': 226,\n",
       "  'here': 227,\n",
       "  'You': 228,\n",
       "  'group': 229,\n",
       "  'really': 230,\n",
       "  'found': 231,\n",
       "  'As': 232,\n",
       "  'used': 233,\n",
       "  '3': 234,\n",
       "  'lot': 235,\n",
       "  \"'m\": 236,\n",
       "  'money': 237,\n",
       "  'put': 238,\n",
       "  'games': 239,\n",
       "  'support': 240,\n",
       "  'program': 241,\n",
       "  'half': 242,\n",
       "  'report': 243,\n",
       "  'family': 244,\n",
       "  'months': 245,\n",
       "  'number': 246,\n",
       "  'officials': 247,\n",
       "  'am': 248,\n",
       "  'former': 249,\n",
       "  'own': 250,\n",
       "  'man': 251,\n",
       "  'Saturday': 252,\n",
       "  'too': 253,\n",
       "  'better': 254,\n",
       "  'days': 255,\n",
       "  'came': 256,\n",
       "  'lead': 257,\n",
       "  'life': 258,\n",
       "  'American': 259,\n",
       "  '##-##': 260,\n",
       "  'show': 261,\n",
       "  'past': 262,\n",
       "  'took': 263,\n",
       "  'added': 264,\n",
       "  'expected': 265,\n",
       "  'called': 266,\n",
       "  'great': 267,\n",
       "  'State': 268,\n",
       "  'services': 269,\n",
       "  'children': 270,\n",
       "  'hit': 271,\n",
       "  'area': 272,\n",
       "  'system': 273,\n",
       "  'every': 274,\n",
       "  'pm': 275,\n",
       "  'big': 276,\n",
       "  'service': 277,\n",
       "  'few': 278,\n",
       "  'per': 279,\n",
       "  'members': 280,\n",
       "  'Sunday': 281,\n",
       "  'early': 282,\n",
       "  'point': 283,\n",
       "  'start': 284,\n",
       "  'companies': 285,\n",
       "  'little': 286,\n",
       "  '&': 287,\n",
       "  'case': 288,\n",
       "  'ago': 289,\n",
       "  'local': 290,\n",
       "  'according': 291,\n",
       "  'never': 292,\n",
       "  '5': 293,\n",
       "  'without': 294,\n",
       "  'sales': 295,\n",
       "  'until': 296,\n",
       "  'went': 297,\n",
       "  'players': 298,\n",
       "  '##th': 299,\n",
       "  'New_York': 300,\n",
       "  'won': 301,\n",
       "  'financial': 302,\n",
       "  'news': 303,\n",
       "  '4': 304,\n",
       "  'When': 305,\n",
       "  'share': 306,\n",
       "  'several': 307,\n",
       "  'free': 308,\n",
       "  'away': 309,\n",
       "  '##.##': 310,\n",
       "  'already': 311,\n",
       "  'On': 312,\n",
       "  'industry': 313,\n",
       "  \"'ll\": 314,\n",
       "  'call': 315,\n",
       "  'With': 316,\n",
       "  'students': 317,\n",
       "  'line': 318,\n",
       "  'available': 319,\n",
       "  'County': 320,\n",
       "  'making': 321,\n",
       "  'held': 322,\n",
       "  'final': 323,\n",
       "  '#:##': 324,\n",
       "  'power': 325,\n",
       "  'plan': 326,\n",
       "  'might': 327,\n",
       "  'least': 328,\n",
       "  'look': 329,\n",
       "  'forward': 330,\n",
       "  'give': 331,\n",
       "  'At': 332,\n",
       "  'again': 333,\n",
       "  'later': 334,\n",
       "  'full': 335,\n",
       "  'must': 336,\n",
       "  'things': 337,\n",
       "  'major': 338,\n",
       "  'community': 339,\n",
       "  'announced': 340,\n",
       "  'open': 341,\n",
       "  'record': 342,\n",
       "  'reported': 343,\n",
       "  'court': 344,\n",
       "  'working': 345,\n",
       "  'able': 346,\n",
       "  'something': 347,\n",
       "  'president': 348,\n",
       "  'meeting': 349,\n",
       "  'keep': 350,\n",
       "  'March': 351,\n",
       "  'future': 352,\n",
       "  'far': 353,\n",
       "  'deal': 354,\n",
       "  'City': 355,\n",
       "  'May': 356,\n",
       "  'development': 357,\n",
       "  'University': 358,\n",
       "  'find': 359,\n",
       "  'times': 360,\n",
       "  'After': 361,\n",
       "  'office': 362,\n",
       "  'led': 363,\n",
       "  'among': 364,\n",
       "  'June': 365,\n",
       "  'increase': 366,\n",
       "  'China': 367,\n",
       "  'John': 368,\n",
       "  'whether': 369,\n",
       "  'cost': 370,\n",
       "  'security': 371,\n",
       "  'job': 372,\n",
       "  'less': 373,\n",
       "  'head': 374,\n",
       "  'seven': 375,\n",
       "  'growth': 376,\n",
       "  'lost': 377,\n",
       "  'pay': 378,\n",
       "  'looking': 379,\n",
       "  'provide': 380,\n",
       "  '6': 381,\n",
       "  'To': 382,\n",
       "  'plans': 383,\n",
       "  'products': 384,\n",
       "  'car': 385,\n",
       "  'recent': 386,\n",
       "  'hard': 387,\n",
       "  'always': 388,\n",
       "  'include': 389,\n",
       "  'women': 390,\n",
       "  'across': 391,\n",
       "  'tax': 392,\n",
       "  'water': 393,\n",
       "  'April': 394,\n",
       "  'continue': 395,\n",
       "  'important': 396,\n",
       "  'different': 397,\n",
       "  'close': 398,\n",
       "  '7': 399,\n",
       "  'One': 400,\n",
       "  'late': 401,\n",
       "  'decision': 402,\n",
       "  'current': 403,\n",
       "  'law': 404,\n",
       "  'within': 405,\n",
       "  'along': 406,\n",
       "  'played': 407,\n",
       "  'move': 408,\n",
       "  'United_States': 409,\n",
       "  'enough': 410,\n",
       "  'become': 411,\n",
       "  'side': 412,\n",
       "  'national': 413,\n",
       "  'Inc.': 414,\n",
       "  'results': 415,\n",
       "  'level': 416,\n",
       "  'loss': 417,\n",
       "  'economic': 418,\n",
       "  'coach': 419,\n",
       "  'near': 420,\n",
       "  'getting': 421,\n",
       "  'price': 422,\n",
       "  'Department': 423,\n",
       "  'event': 424,\n",
       "  'fourth': 425,\n",
       "  'change': 426,\n",
       "  'All': 427,\n",
       "  'small': 428,\n",
       "  'board': 429,\n",
       "  'National': 430,\n",
       "  'So': 431,\n",
       "  'goal': 432,\n",
       "  'taken': 433,\n",
       "  'field': 434,\n",
       "  'prices': 435,\n",
       "  'weeks': 436,\n",
       "  'men': 437,\n",
       "  'asked': 438,\n",
       "  'eight': 439,\n",
       "  'data': 440,\n",
       "  'shot': 441,\n",
       "  'New': 442,\n",
       "  'started': 443,\n",
       "  'July': 444,\n",
       "  'director': 445,\n",
       "  'President': 446,\n",
       "  'party': 447,\n",
       "  'federal': 448,\n",
       "  'done': 449,\n",
       "  'political': 450,\n",
       "  'minutes': 451,\n",
       "  'taking': 452,\n",
       "  'Company': 453,\n",
       "  'technology': 454,\n",
       "  'project': 455,\n",
       "  'center': 456,\n",
       "  'leading': 457,\n",
       "  'issue': 458,\n",
       "  'though': 459,\n",
       "  'having': 460,\n",
       "  'period': 461,\n",
       "  'likely': 462,\n",
       "  'scored': 463,\n",
       "  '8': 464,\n",
       "  'strong': 465,\n",
       "  'series': 466,\n",
       "  'military': 467,\n",
       "  'seen': 468,\n",
       "  'trying': 469,\n",
       "  'What': 470,\n",
       "  'coming': 471,\n",
       "  'process': 472,\n",
       "  'building': 473,\n",
       "  'behind': 474,\n",
       "  'performance': 475,\n",
       "  'management': 476,\n",
       "  'Iraq': 477,\n",
       "  'saying': 478,\n",
       "  'earlier': 479,\n",
       "  'believe': 480,\n",
       "  'oil': 481,\n",
       "  'given': 482,\n",
       "  'Police': 483,\n",
       "  'customers': 484,\n",
       "  'due': 485,\n",
       "  'following': 486,\n",
       "  'term': 487,\n",
       "  'others': 488,\n",
       "  'statement': 489,\n",
       "  'international': 490,\n",
       "  'economy': 491,\n",
       "  'health': 492,\n",
       "  'thing': 493,\n",
       "  'Obama': 494,\n",
       "  'return': 495,\n",
       "  'killed': 496,\n",
       "  'Washington': 497,\n",
       "  'further': 498,\n",
       "  'However': 499,\n",
       "  'doing': 500,\n",
       "  'face': 501,\n",
       "  'low': 502,\n",
       "  'higher': 503,\n",
       "  'site': 504,\n",
       "  'once': 505,\n",
       "  'yet': 506,\n",
       "  'hours': 507,\n",
       "  'America': 508,\n",
       "  'control': 509,\n",
       "  'received': 510,\n",
       "  'rate': 511,\n",
       "  'career': 512,\n",
       "  'Bush': 513,\n",
       "  'teams': 514,\n",
       "  'known': 515,\n",
       "  'offer': 516,\n",
       "  'race': 517,\n",
       "  'ever': 518,\n",
       "  'experience': 519,\n",
       "  'playing': 520,\n",
       "  'name': 521,\n",
       "  'possible': 522,\n",
       "  'countries': 523,\n",
       "  'Mr.': 524,\n",
       "  'average': 525,\n",
       "  'together': 526,\n",
       "  'using': 527,\n",
       "  '9': 528,\n",
       "  'cut': 529,\n",
       "  'While': 530,\n",
       "  'total': 531,\n",
       "  'round': 532,\n",
       "  'young': 533,\n",
       "  'nearly': 534,\n",
       "  'shares': 535,\n",
       "  'member': 536,\n",
       "  'campaign': 537,\n",
       "  'media': 538,\n",
       "  'needs': 539,\n",
       "  'why': 540,\n",
       "  'house': 541,\n",
       "  'issues': 542,\n",
       "  'costs': 543,\n",
       "  'fire': 544,\n",
       "  '##-#': 545,\n",
       "  'victory': 546,\n",
       "  'player': 547,\n",
       "  'began': 548,\n",
       "  'sure': 549,\n",
       "  'story': 550,\n",
       "  'per_cent': 551,\n",
       "  'North': 552,\n",
       "  'His': 553,\n",
       "  'staff': 554,\n",
       "  'order': 555,\n",
       "  'war': 556,\n",
       "  'large': 557,\n",
       "  'interest': 558,\n",
       "  'stock': 559,\n",
       "  'food': 560,\n",
       "  'research': 561,\n",
       "  'key': 562,\n",
       "  'India': 563,\n",
       "  'South': 564,\n",
       "  'morning': 565,\n",
       "  'conference': 566,\n",
       "  'senior': 567,\n",
       "  'global': 568,\n",
       "  'Center': 569,\n",
       "  'death': 570,\n",
       "  'person': 571,\n",
       "  'thought': 572,\n",
       "  'gave': 573,\n",
       "  'feel': 574,\n",
       "  'energy': 575,\n",
       "  'history': 576,\n",
       "  'recently': 577,\n",
       "  'largest': 578,\n",
       "  'No.': 579,\n",
       "  'general': 580,\n",
       "  'official': 581,\n",
       "  'released': 582,\n",
       "  'wanted': 583,\n",
       "  'meet': 584,\n",
       "  'short': 585,\n",
       "  'outside': 586,\n",
       "  'running': 587,\n",
       "  'live': 588,\n",
       "  'ball': 589,\n",
       "  'online': 590,\n",
       "  'real': 591,\n",
       "  'position': 592,\n",
       "  'fact': 593,\n",
       "  'fell': 594,\n",
       "  'nine': 595,\n",
       "  'December': 596,\n",
       "  'front': 597,\n",
       "  'action': 598,\n",
       "  'defense': 599,\n",
       "  'problem': 600,\n",
       "  'problems': 601,\n",
       "  'Mr': 602,\n",
       "  'nation': 603,\n",
       "  'needed': 604,\n",
       "  'special': 605,\n",
       "  'January': 606,\n",
       "  'almost': 607,\n",
       "  'chance': 608,\n",
       "  \"'d\": 609,\n",
       "  'result': 610,\n",
       "  'West': 611,\n",
       "  'September': 612,\n",
       "  'reports': 613,\n",
       "  'leader': 614,\n",
       "  'investment': 615,\n",
       "  'yesterday': 616,\n",
       "  'Some': 617,\n",
       "  'leaders': 618,\n",
       "  'ahead': 619,\n",
       "  'production': 620,\n",
       "  'comes': 621,\n",
       "  'No': 622,\n",
       "  'runs': 623,\n",
       "  'match': 624,\n",
       "  'role': 625,\n",
       "  'kind': 626,\n",
       "  'try': 627,\n",
       "  'ended': 628,\n",
       "  'risk': 629,\n",
       "  'areas': 630,\n",
       "  'election': 631,\n",
       "  'workers': 632,\n",
       "  'visit': 633,\n",
       "  'bring': 634,\n",
       "  'road': 635,\n",
       "  'music': 636,\n",
       "  'study': 637,\n",
       "  'makes': 638,\n",
       "  'often': 639,\n",
       "  'release': 640,\n",
       "  'woman': 641,\n",
       "  'vote': 642,\n",
       "  'care': 643,\n",
       "  'town': 644,\n",
       "  'clear': 645,\n",
       "  'comment': 646,\n",
       "  'budget': 647,\n",
       "  'potential': 648,\n",
       "  'single': 649,\n",
       "  'markets': 650,\n",
       "  'policy': 651,\n",
       "  'capital': 652,\n",
       "  'saw': 653,\n",
       "  'access': 654,\n",
       "  'weekend': 655,\n",
       "  'operations': 656,\n",
       "  'whose': 657,\n",
       "  'net': 658,\n",
       "  'House': 659,\n",
       "  'hand': 660,\n",
       "  'increased': 661,\n",
       "  'charges': 662,\n",
       "  'winning': 663,\n",
       "  'trade': 664,\n",
       "  'These': 665,\n",
       "  'income': 666,\n",
       "  'value': 667,\n",
       "  'involved': 668,\n",
       "  'Bank': 669,\n",
       "  'November': 670,\n",
       "  'bill': 671,\n",
       "  'compared': 672,\n",
       "  'anything': 673,\n",
       "  'manager': 674,\n",
       "  'Texas': 675,\n",
       "  'property': 676,\n",
       "  'stop': 677,\n",
       "  'annual': 678,\n",
       "  'private': 679,\n",
       "  'contract': 680,\n",
       "  'died': 681,\n",
       "  'Now': 682,\n",
       "  'hope': 683,\n",
       "  'product': 684,\n",
       "  'fans': 685,\n",
       "  'lower': 686,\n",
       "  'demand': 687,\n",
       "  'News': 688,\n",
       "  'David': 689,\n",
       "  'club': 690,\n",
       "  'comments': 691,\n",
       "  'film': 692,\n",
       "  'yards': 693,\n",
       "  'quality': 694,\n",
       "  'currently': 695,\n",
       "  'events': 696,\n",
       "  'addition': 697,\n",
       "  'couple': 698,\n",
       "  'schools': 699,\n",
       "  'attack': 700,\n",
       "  'region': 701,\n",
       "  'latest': 702,\n",
       "  'opportunity': 703,\n",
       "  'worked': 704,\n",
       "  'course': 705,\n",
       "  'bad': 706,\n",
       "  'fall': 707,\n",
       "  'Group': 708,\n",
       "  'October': 709,\n",
       "  'jobs': 710,\n",
       "  'list': 711,\n",
       "  'let': 712,\n",
       "  'however': 713,\n",
       "  'chief': 714,\n",
       "  'summer': 715,\n",
       "  'programs': 716,\n",
       "  'According': 717,\n",
       "  'revenue': 718,\n",
       "  'Our': 719,\n",
       "  'rose': 720,\n",
       "  'previous': 721,\n",
       "  'TV': 722,\n",
       "  'football': 723,\n",
       "  'biggest': 724,\n",
       "  'employees': 725,\n",
       "  'changes': 726,\n",
       "  'residents': 727,\n",
       "  'means': 728,\n",
       "  'agreement': 729,\n",
       "  'includes': 730,\n",
       "  'post': 731,\n",
       "  'Canada': 732,\n",
       "  'probably': 733,\n",
       "  'related': 734,\n",
       "  'training': 735,\n",
       "  'allowed': 736,\n",
       "  'class': 737,\n",
       "  'bit': 738,\n",
       "  'video': 739,\n",
       "  'Michael': 740,\n",
       "  'An': 741,\n",
       "  'sent': 742,\n",
       "  'education': 743,\n",
       "  'states': 744,\n",
       "  'straight': 745,\n",
       "  'love': 746,\n",
       "  'beat': 747,\n",
       "  'hold': 748,\n",
       "  'turn': 749,\n",
       "  'finished': 750,\n",
       "  'network': 751,\n",
       "  'Smith': 752,\n",
       "  'buy': 753,\n",
       "  'foreign': 754,\n",
       "  'especially': 755,\n",
       "  'groups': 756,\n",
       "  'wants': 757,\n",
       "  'title': 758,\n",
       "  'included': 759,\n",
       "  'turned': 760,\n",
       "  'bank': 761,\n",
       "  'Florida': 762,\n",
       "  'efforts': 763,\n",
       "  'personal': 764,\n",
       "  'businesses': 765,\n",
       "  'August': 766,\n",
       "  'California': 767,\n",
       "  'situation': 768,\n",
       "  'district': 769,\n",
       "  'allow': 770,\n",
       "  'helped': 771,\n",
       "  'body': 772,\n",
       "  'nothing': 773,\n",
       "  'soon': 774,\n",
       "  'safety': 775,\n",
       "  'officer': 776,\n",
       "  'cents': 777,\n",
       "  'Europe': 778,\n",
       "  'St.': 779,\n",
       "  'additional': 780,\n",
       "  'spokesman': 781,\n",
       "  'February': 782,\n",
       "  'wife': 783,\n",
       "  'showed': 784,\n",
       "  'leave': 785,\n",
       "  'investors': 786,\n",
       "  'parents': 787,\n",
       "  'medical': 788,\n",
       "  'spending': 789,\n",
       "  'non': 790,\n",
       "  'London': 791,\n",
       "  'Council': 792,\n",
       "  'matter': 793,\n",
       "  'spent': 794,\n",
       "  'child': 795,\n",
       "  'World': 796,\n",
       "  'effort': 797,\n",
       "  'opening': 798,\n",
       "  'either': 799,\n",
       "  'range': 800,\n",
       "  'question': 801,\n",
       "  'European': 802,\n",
       "  'goals': 803,\n",
       "  'administration': 804,\n",
       "  'friends': 805,\n",
       "  'himself': 806,\n",
       "  'shows': 807,\n",
       "  'difficult': 808,\n",
       "  'kids': 809,\n",
       "  'paid': 810,\n",
       "  'create': 811,\n",
       "  'cash': 812,\n",
       "  'age': 813,\n",
       "  'league': 814,\n",
       "  'form': 815,\n",
       "  'impact': 816,\n",
       "  'drive': 817,\n",
       "  'someone': 818,\n",
       "  'became': 819,\n",
       "  'stay': 820,\n",
       "  'fight': 821,\n",
       "  'significant': 822,\n",
       "  'firm': 823,\n",
       "  'Senate': 824,\n",
       "  'hospital': 825,\n",
       "  'charged': 826,\n",
       "  'operating': 827,\n",
       "  'main': 828,\n",
       "  'book': 829,\n",
       "  'success': 830,\n",
       "  'son': 831,\n",
       "  'trading': 832,\n",
       "  '###-####': 833,\n",
       "  'focus': 834,\n",
       "  'room': 835,\n",
       "  'continued': 836,\n",
       "  'Congress': 837,\n",
       "  'everything': 838,\n",
       "  'Park': 839,\n",
       "  'agency': 840,\n",
       "  'brought': 841,\n",
       "  'talk': 842,\n",
       "  'break': 843,\n",
       "  'air': 844,\n",
       "  'software': 845,\n",
       "  'decided': 846,\n",
       "  'Do': 847,\n",
       "  'ready': 848,\n",
       "  'arrested': 849,\n",
       "  'track': 850,\n",
       "  'provides': 851,\n",
       "  'mother': 852,\n",
       "  'base': 853,\n",
       "  'trial': 854,\n",
       "  'phone': 855,\n",
       "  'My': 856,\n",
       "  'build': 857,\n",
       "  'conditions': 858,\n",
       "  'rest': 859,\n",
       "  'Johnson': 860,\n",
       "  'terms': 861,\n",
       "  'expect': 862,\n",
       "  'England': 863,\n",
       "  'Israel': 864,\n",
       "  'despite': 865,\n",
       "  'closed': 866,\n",
       "  'starting': 867,\n",
       "  'provided': 868,\n",
       "  'pressure': 869,\n",
       "  'lives': 870,\n",
       "  'step': 871,\n",
       "  'remain': 872,\n",
       "  'similar': 873,\n",
       "  'charge': 874,\n",
       "  'date': 875,\n",
       "  'whole': 876,\n",
       "  'land': 877,\n",
       "  'growing': 878,\n",
       "  'James': 879,\n",
       "  'Internet': 880,\n",
       "  'projects': 881,\n",
       "  'British': 882,\n",
       "  'cases': 883,\n",
       "  'ground': 884,\n",
       "  'legal': 885,\n",
       "  'International': 886,\n",
       "  'agreed': 887,\n",
       "  'tell': 888,\n",
       "  'test': 889,\n",
       "  'everyone': 890,\n",
       "  'pretty': 891,\n",
       "  'authorities': 892,\n",
       "  'Two': 893,\n",
       "  'above': 894,\n",
       "  'moved': 895,\n",
       "  'profit': 896,\n",
       "  'throughout': 897,\n",
       "  'inside': 898,\n",
       "  'ability': 899,\n",
       "  'overall': 900,\n",
       "  'pass': 901,\n",
       "  'officers': 902,\n",
       "  'rather': 903,\n",
       "  'Australia': 904,\n",
       "  'actually': 905,\n",
       "  'county': 906,\n",
       "  'amount': 907,\n",
       "  'scheduled': 908,\n",
       "  'themselves': 909,\n",
       "  'organization': 910,\n",
       "  'giving': 911,\n",
       "  'credit': 912,\n",
       "  'father': 913,\n",
       "  'drug': 914,\n",
       "  'investigation': 915,\n",
       "  'families': 916,\n",
       "  'Republican': 917,\n",
       "  'funds': 918,\n",
       "  'patients': 919,\n",
       "  'takes': 920,\n",
       "  'systems': 921,\n",
       "  'Japan': 922,\n",
       "  'complete': 923,\n",
       "  'sold': 924,\n",
       "  'practice': 925,\n",
       "  'calls': 926,\n",
       "  '•': 927,\n",
       "  'UK': 928,\n",
       "  'force': 929,\n",
       "  'student': 930,\n",
       "  'idea': 931,\n",
       "  'reached': 932,\n",
       "  'reason': 933,\n",
       "  'levels': 934,\n",
       "  'space': 935,\n",
       "  'competition': 936,\n",
       "  'forces': 937,\n",
       "  'sector': 938,\n",
       "  'Last': 939,\n",
       "  'tried': 940,\n",
       "  'common': 941,\n",
       "  'homes': 942,\n",
       "  'stage': 943,\n",
       "  'department': 944,\n",
       "  'named': 945,\n",
       "  'earnings': 946,\n",
       "  'offers': 947,\n",
       "  'star': 948,\n",
       "  'certain': 949,\n",
       "  'double': 950,\n",
       "  'longer': 951,\n",
       "  'followed': 952,\n",
       "  'cause': 953,\n",
       "  'Association': 954,\n",
       "  'signed': 955,\n",
       "  'committee': 956,\n",
       "  'hour': 957,\n",
       "  'college': 958,\n",
       "  'Pakistan': 959,\n",
       "  'users': 960,\n",
       "  'Iran': 961,\n",
       "  'sign': 962,\n",
       "  'living': 963,\n",
       "  'failed': 964,\n",
       "  'reach': 965,\n",
       "  'quickly': 966,\n",
       "  'receive': 967,\n",
       "  'debt': 968,\n",
       "  'sale': 969,\n",
       "  'Board': 970,\n",
       "  'Americans': 971,\n",
       "  'Road': 972,\n",
       "  'Brown': 973,\n",
       "  'insurance': 974,\n",
       "  '##:##': 975,\n",
       "  'anyone': 976,\n",
       "  'tournament': 977,\n",
       "  'More': 978,\n",
       "  'gas': 979,\n",
       "  'talks': 980,\n",
       "  'serious': 981,\n",
       "  'required': 982,\n",
       "  'sell': 983,\n",
       "  'construction': 984,\n",
       "  'evidence': 985,\n",
       "  'remains': 986,\n",
       "  'black': 987,\n",
       "  'below': 988,\n",
       "  'improve': 989,\n",
       "  'crisis': 990,\n",
       "  'address': 991,\n",
       "  'questions': 992,\n",
       "  'easy': 993,\n",
       "  'begin': 994,\n",
       "  'view': 995,\n",
       "  'School': 996,\n",
       "  'heard': 997,\n",
       "  'executive': 998,\n",
       "  'raised': 999,\n",
       "  ...},\n",
       " tensor([[ 1.1292e-03, -8.9645e-04,  3.1853e-04,  ..., -1.5640e-03,\n",
       "          -1.2302e-04, -8.6308e-05],\n",
       "         [ 7.0312e-02,  8.6914e-02,  8.7891e-02,  ..., -4.7607e-02,\n",
       "           1.4465e-02, -6.2500e-02],\n",
       "         [-1.1780e-02, -4.7363e-02,  4.4678e-02,  ...,  7.1289e-02,\n",
       "          -3.4912e-02,  2.4170e-02],\n",
       "         ...,\n",
       "         [ 3.2715e-02, -3.2227e-02,  3.6133e-02,  ..., -8.8501e-03,\n",
       "           2.6978e-02,  1.9043e-02],\n",
       "         [ 4.5166e-02, -4.5166e-02, -3.9368e-03,  ...,  7.9590e-02,\n",
       "           7.2266e-02,  1.3000e-02],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]], device='cuda:0'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_word2vec(word2vec_embeddings, embedding_components) -> tuple[dict[str, int], torch.Tensor]:\n",
    "        word_labels: dict[str, int] = {}\n",
    "        tensor: torch.Tensor = torch.empty((EMBED_SIZE + 1, embedding_components), dtype=torch.float32, device=device)\n",
    "        \n",
    "        # Clean up the file and load the embeddings into a tensor\n",
    "        loop_idx = 0\n",
    "        for word, idx in word2vec_embeddings.key_to_index.items():\n",
    "            word_labels[word] = idx\n",
    "            tensor[idx] = torch.tensor(word2vec_embeddings.get_vector(word), dtype=torch.float32,\n",
    "                                         device=device)\n",
    "            # Output our progress every 100,000 words\n",
    "            if (loop_idx + 1) % 100000 == 0:\n",
    "                print(\"Processed {}/{}\".format(loop_idx + 1, EMBED_SIZE))\n",
    "            loop_idx += 1\n",
    "        tensor[-1] = torch.zeros(embedding_components, dtype=torch.float32, device=device)\n",
    "\n",
    "        # Adding a padding token\n",
    "        word_labels[\"<PAD>\"] = EMBED_SIZE\n",
    "        tensor.to(device)\n",
    "        return word_labels, tensor\n",
    "\n",
    "\"\"\"Deserialize the embeddings, and return word labels with their corresponding tensors.\"\"\"\n",
    "def get_vectors(embedding: str) -> tuple[dict[str, int], torch.Tensor]:\n",
    "    skip_first_line: bool = False\n",
    "    global EMBED_SIZE\n",
    "    match embedding:\n",
    "        case \"glove\":\n",
    "            embedding_path: str = EMBEDDINGS_PATH + \"glove.840B.300d.txt\"\n",
    "            EMBED_SIZE = 2196018\n",
    "            embedding_components: int = 300\n",
    "        case \"word2vec\":\n",
    "            embedding_path: str = EMBEDDINGS_PATH + \"GoogleNews-vectors-negative300.bin\"\n",
    "            gn_model = KeyedVectors.load_word2vec_format(embedding_path, binary=True)\n",
    "            embedding_components: int = 300\n",
    "            EMBED_SIZE = 3000000\n",
    "            return parse_word2vec(gn_model, embedding_components)\n",
    "        case \"numberbatch\":\n",
    "            embedding_path: str = EMBEDDINGS_PATH + \"numberbatch-19.08-en.txt\"\n",
    "            EMBED_SIZE = 516782\n",
    "            embedding_components: int = 300\n",
    "            skip_first_line = True\n",
    "        case default:\n",
    "            raise RuntimeError(\"Invalid embedding chosen.\")\n",
    "        \n",
    "    # Deserializing glove and numberbatch embeddings\n",
    "    if not os.path.exists(embedding_path):\n",
    "        raise FileNotFoundError(\"Could not find embedding file: {}\".format(embedding_path))\n",
    "    with (open(embedding_path, encoding=\"utf_8\") as embeddings_file):\n",
    "        word_labels: dict[str, int] = {}\n",
    "        tensor: torch.Tensor = torch.empty((EMBED_SIZE + 1, embedding_components), dtype=torch.float32, device=device)\n",
    "        \n",
    "        # We need to skip the first line of the numberbatch embeddings because that's header information\n",
    "        if skip_first_line:\n",
    "            _ = embeddings_file.readline()\n",
    "        \n",
    "        # Clean up the file and load the embeddings into a tensor\n",
    "        for index, embedding in enumerate(embeddings_file):\n",
    "            embedding_split: list[str] = embedding.rstrip().split(\" \")\n",
    "            word_labels[embedding_split[0]] = index # Assign the word to the index\n",
    "            tensor[index] = torch.tensor([float(val) for val in embedding_split[1:]], dtype=torch.float32, # Every element except the first is converted to a float\n",
    "                                         device=device)\n",
    "            # Output our progress every 100,000 words\n",
    "            if (index + 1) % 100000 == 0:\n",
    "                print(\"Processed {}/{}\".format(index + 1, EMBED_SIZE))\n",
    "        tensor[-1] = torch.zeros(embedding_components, dtype=torch.float32, device=device)\n",
    "\n",
    "        # Adding a padding token\n",
    "        word_labels[\"<PAD>\"] = EMBED_SIZE\n",
    "        tensor.to(device)\n",
    "        return word_labels, tensor\n",
    "\n",
    "\"\"\" Tokenize the text and convert it into a list of integers. We'll use a dictionary to map words to integers. \n",
    "    We'll also use a special token (\"something\") for words that are not in the dictionary.\n",
    "    The tokenizer function splits the text into words.\"\"\"\n",
    "def tokenize(text: str, labels: dict, tokenizer: FunctionType) -> list[int]:\n",
    "    return [labels[word] if word in labels.keys() else labels[\"something\"] for word in tokenizer(text)]\n",
    "\n",
    "\n",
    "def resolve_emotions(id: str) -> str:\n",
    "    return [emotions[int(emotion)] for emotion in id.split(\",\")]\n",
    "\n",
    "\"\"\" \n",
    "Train a Bidirectional RNN model \n",
    "\"\"\"\n",
    "def train(model: BDRNN, batches, num_epochs: int):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    losses = []\n",
    "\n",
    "    model.train()\n",
    "    for epoch_num, epochs in enumerate(range(num_epochs)):\n",
    "        correct: int = 0\n",
    "        total: int = 0\n",
    "        for num_batch, batch in enumerate(batches):\n",
    "            for sentence, emotions in batch:\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                predictions = model(sentence)\n",
    "                # Rounding is naive, we should base this off a confidence threshold\n",
    "                guesses = torch.round(torch.sigmoid(predictions))\n",
    "                if torch.equal(guesses, emotions): correct += 1\n",
    "                total += 1\n",
    "\n",
    "                loss = criterion(predictions, emotions)\n",
    "                losses.append(float(loss))\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "        print(\"Epoch: {} | Loss: {} | Accuracy: {}%\".format(epoch_num + 1, sum(losses) / len(losses), (correct /\n",
    "                                                                                                      total) * 100))\n",
    "\n",
    "def collate(batch: list[tuple[list[int], list[str]]]) -> list[tuple[torch.IntTensor, torch.Tensor]]:\n",
    "    final_batch = []\n",
    "    max_tokens = len(max(batch, key=lambda tuple: len(tuple[0]))[0])\n",
    "    for sentence, emotions in batch:\n",
    "        sentence.extend([EMBED_SIZE] * (max_tokens - len(sentence)))\n",
    "        sentence = torch.IntTensor([int(value) for value in sentence]).to(device)\n",
    "        _emotions = torch.zeros(NUM_EMOTIONS, dtype=torch.float32, device=device)\n",
    "        emotions = emotions.split(\",\")\n",
    "        for emotion in emotions:\n",
    "            _emotions[int(emotion)] = 1.0\n",
    "        final_batch.append((sentence, _emotions))\n",
    "    return final_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text emotion_ids\n",
      "0  My favourite food is anything I didn't have to...          27\n",
      "1  Now if he does off himself, everyone will thin...          27\n",
      "2                     WHY THE FUCK IS BAYLESS ISOING           2\n",
      "3                        To make her feel threatened          14\n",
      "4                             Dirty Southern Wankers           3\n",
      "                                                text emotion_ids\n",
      "0  I’m really sorry about your situation :( Altho...          25\n",
      "1    It's wonderful because it's awful. At not with.           0\n",
      "2  Kings fan here, good luck to you guys! Will be...          13\n",
      "3  I didn't know that, thank you for teaching me ...          15\n",
      "4  They got bored from haunting earth for thousan...          27\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m     train(numberbatch_model, train_dataloader, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 34\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 18\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m input_dim: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(math\u001b[38;5;241m.\u001b[39mlog2(max_words)) \u001b[38;5;28;01mif\u001b[39;00m max_words \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Time to do some training!\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m labels, vectors \u001b[38;5;241m=\u001b[39m \u001b[43mget_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnumberbatch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m torchtext\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mget_tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbasic_english\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m training_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m training_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(tokenize, labels\u001b[38;5;241m=\u001b[39mlabels, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "Cell \u001b[0;32mIn[19], line 29\u001b[0m, in \u001b[0;36mget_vectors\u001b[0;34m(embedding)\u001b[0m\n\u001b[1;32m     26\u001b[0m tensor: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty((EMBED_SIZE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, embedding_components), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# We need to skip the first line of the numberbatch embeddings because that's header information\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mskip_first_line\u001b[49m:\n\u001b[1;32m     30\u001b[0m     _ \u001b[38;5;241m=\u001b[39m embeddings_file\u001b[38;5;241m.\u001b[39mreadline()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Clean up the file and load the embeddings into a tensor\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 29\u001b[0m, in \u001b[0;36mget_vectors\u001b[0;34m(embedding)\u001b[0m\n\u001b[1;32m     26\u001b[0m tensor: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty((EMBED_SIZE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, embedding_components), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# We need to skip the first line of the numberbatch embeddings because that's header information\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mskip_first_line\u001b[49m:\n\u001b[1;32m     30\u001b[0m     _ \u001b[38;5;241m=\u001b[39m embeddings_file\u001b[38;5;241m.\u001b[39mreadline()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Clean up the file and load the embeddings into a tensor\u001b[39;00m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1395\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1344\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/8321-Mach-Lrng-Neural-Ntwrks/remote-venv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/8321-Mach-Lrng-Neural-Ntwrks/remote-venv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Now we need to handle our dataset\n",
    "    with open(DATASET_PATH + \"emotions.txt\") as emotions_file:\n",
    "        emotions = [emotion.strip() for emotion in emotions_file]\n",
    "    if len(emotions) != NUM_EMOTIONS or emotions[4] != \"approval\":\n",
    "        raise RuntimeError(\"Failed to load emotion mappings.\")\n",
    "\n",
    "    training_set = pd.read_csv(DATASET_PATH + \"train.tsv\", delimiter=\"\\t\", names=[\"text\", \"emotion_ids\"],\n",
    "                               usecols=[0, 1])\n",
    "    testing_set = pd.read_csv(DATASET_PATH + \"test.tsv\", delimiter=\"\\t\", usecols=[0, 1])\n",
    "    print(training_set.head())\n",
    "    print(testing_set.head())\n",
    "\n",
    "    max_words: int = max(training_set[\"text\"].map(len).max(), testing_set[\"text\"].map(len).max())\n",
    "    input_dim: int = 2 ** math.ceil(math.log2(max_words)) if max_words >= 2 else 2\n",
    "\n",
    "    # Time to do some training!\n",
    "    labels, vectors = get_vectors(\"numberbatch\")\n",
    "    tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")\n",
    "    training_set[\"text\"] = training_set[\"text\"].apply(tokenize, labels=labels, tokenizer=tokenizer)\n",
    "    testing_set[\"text\"] = testing_set[\"text\"].apply(tokenize, labels=labels, tokenizer=tokenizer)\n",
    "    print(training_set.head())\n",
    "    print(testing_set.head())\n",
    "    numberbatch_model = BDRNN(vectors.shape[1], vectors, NUM_EMOTIONS, 4, 0.5).to(device)\n",
    "    train_dataset = pandas_dataset(training_set)\n",
    "    test_dataset = pandas_dataset(testing_set)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "    print('Created `training dataloader` with %d batches!' % len(train_dataloader))\n",
    "    print('Created `testing dataloader` with %d batches!' % len(test_dataloader))\n",
    "    train(numberbatch_model, train_dataloader, 10)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download link for word2vec: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n",
    "# Download link for Glove: https://huggingface.co/stanfordnlp/glove/resolve/main/glove.840B.300d.zip\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "from sklearn.manifold import TSNE\n",
    "from torch import Tensor, cat\n",
    "from torch.cuda import is_available as cuda_is_available\n",
    "from train import get_vectors\n",
    "\n",
    "\n",
    "# EDITABLE VARIABLES\n",
    "embeddings = [\"numberbatch\", \"glove\"] # Embeddings definition. Add word2vec once deserialization is done.\n",
    "\n",
    "# Word comparison groups. Format: [base_word, similar_word_1, similar_word_2]\n",
    "word_comparison_groups = [\n",
    "    [\"tire\", \"tired\", \"tyre\"],\n",
    "]\n",
    "\n",
    "# Add a new distance function here if you want.\n",
    "\"\"\"Calculate the distances between a base word and two similar words using Euclidean, Cosine, and Manhattan distances.\"\"\"\n",
    "def calculate_distances(base_word: ndarray[float], similar_word_1: ndarray[float], similar_word_2: ndarray[float]) -> dict[str, list[float]]:\n",
    "    return {\n",
    "        \"euclidean\": [euclidean_distance(similar_word_1, base_word), euclidean_distance(similar_word_2, base_word)],\n",
    "        \"cosine\": [cosine_similarity(similar_word_1, base_word), cosine_similarity(similar_word_2, base_word)],\n",
    "        \"manhattan\": [manhattan_distance(similar_word_1, base_word), manhattan_distance(similar_word_2, base_word)],\n",
    "    }\n",
    "\n",
    "# We should look at comparing vectors in different embeddings and see how well ambigious words center around common\n",
    "# synonyms for each meaning. We could probably do some sort of visualization for this as well.\n",
    "\n",
    "def euclidean_distance(vector1: Tensor, vector2: Tensor) -> float:\n",
    "    return np.linalg.norm(vector1 - vector2)\n",
    "\n",
    "def cosine_similarity(vector1: Tensor, vector2: Tensor) -> float:\n",
    "    return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "def manhattan_distance(v1, v2):\n",
    "    return np.sum(np.abs(v1 - v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Converts the given list of tensors to a numpy array based on GPU availability.\"\"\"\n",
    "def convert_tensors_to_numpy(embeddings_list: list[Tensor]) -> ndarray[float]:\n",
    "    if cuda_is_available():\n",
    "        numpy_vectors = np.array([vector.cpu().numpy() for vector in embeddings_list])\n",
    "    else:\n",
    "        numpy_vectors = np.array([vector.numpy() for vector in embeddings_list])\n",
    "    return numpy_vectors\n",
    "\n",
    "\"\"\"Returns the word vectors to compare from the given numpy vectors. These vectors are a word groups members\"\"\"\n",
    "def get_word_vectors_to_compare(numpy_vectors: ndarray[float]) -> tuple[ndarray[float], ndarray[float], ndarray[float]]:\n",
    "    return numpy_vectors[0], numpy_vectors[1], numpy_vectors[2]\n",
    "\n",
    "\"\"\"Returns a dictionary of distance types to their calculated distances for the given word vectors.\"\"\"\n",
    "def get_distances(embeddings_list: list[Tensor]) -> dict[str, list[float]]:\n",
    "    numpy_vectors = convert_tensors_to_numpy(embeddings_list)\n",
    "    base_word, similar_word1, similar_word2 = get_word_vectors_to_compare(numpy_vectors)\n",
    "    return calculate_distances(base_word, similar_word1, similar_word2)\n",
    "\n",
    "\"\"\"Returns a dictionary of embedding names to their respective word vectors and vocabularies\"\"\"\n",
    "def get_embeddings(embeddings: list[str]) -> dict[str, (dict[str, int], Tensor)]:\n",
    "    if len(embeddings) == 0:\n",
    "        raise ValueError(\"No embeddings were selected to load.\")\n",
    "    \n",
    "    result = {}\n",
    "    for embedding in embeddings:\n",
    "        vocab, vectors = get_vectors(embedding)\n",
    "        result[embedding] = (vocab, vectors)\n",
    "    return result\n",
    "\n",
    "\"\"\"Returns a dictionary of embedding names to a list of comparison groups, the words whose distances are being compared\"\"\"\n",
    "def get_comparison_embeddings(embeddings: dict[str, (dict[str, int], Tensor)]) -> dict[str, list[list[Tensor]]]:\n",
    "    # Dictionary of embedding name to list of comparison groups\n",
    "    result: dict[str, list[list[Tensor]]] = {}\n",
    "    for embed_name, (vocab, vectors) in embeddings.items():\n",
    "        comparisons = []\n",
    "        if embed_name not in result:\n",
    "            result[embed_name] = []\n",
    "        \n",
    "        # Populate the comparison groups\n",
    "        for idx, group in enumerate(word_comparison_groups):\n",
    "            comparisons = []\n",
    "            for word in group:\n",
    "                comparisons.append(vectors[vocab[word]])\n",
    "            result[embed_name].append(comparisons)\n",
    "    return result\n",
    "\n",
    "\"\"\"Returns a dictionary of embedding names to a list of dictionaries of distance types to their calculated distances\"\"\"\n",
    "def compare_embeddings(comparison_embeddings: dict[str, list[list[Tensor]]]) -> dict[str, list[dict[str, list[float]]]]:\n",
    "    result = {}\n",
    "    for embedding, word_groups in comparison_embeddings.items():\n",
    "        for idx, group_vectors in enumerate(word_groups):\n",
    "            if result.get(embedding) is None:\n",
    "                result[embedding] = []\n",
    "            distances: dict[str, list[float]] = get_distances(group_vectors)\n",
    "            result[embedding].append(distances)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Plots boxplots of the distance ratios for each embedding and distance type.\"\"\"\n",
    "def compare_distances(embedding_distances: dict[str, list[dict[str, list[float]]]]):\n",
    "    \"\"\"Returns a dictionary of embedding names to a dictionary of distance types to their calculated distance ratios.\"\"\"\n",
    "    def calculate_distance_ratios():\n",
    "        # Dict of embedding name to distance type and all of that distance type's calculated distance ratios\n",
    "        ratios: dict[str, dict[str, list[float]]] = {}\n",
    "        # For every distance type for embeddings\n",
    "        for embedding_name, distances in embedding_distances.items():\n",
    "            ratios[embedding_name] = {}\n",
    "            for distance in distances:\n",
    "                for distance_type, values in distance.items():\n",
    "                    if ratios[embedding_name].get(distance_type) is None:\n",
    "                        ratios[embedding_name][distance_type] = []\n",
    "                    ratio = (max(values[0], values[1]) / min(values[0], values[1]))\n",
    "                    ratios[embedding_name][distance_type].append(ratio)\n",
    "        return ratios\n",
    "\n",
    "    \"\"\"\n",
    "    Plots boxplots of the distance ratios for each embedding and distance type.\n",
    "    Args: ratios: dict[str, dict[str, list[float]]] - Dictionary of embedding names to a dictionary of distance types to a list of all their calculated distance ratios.\n",
    "    \n",
    "    Ex:\n",
    "    ratios = {\n",
    "        \"glove\": {\n",
    "            \"euclidean\": [1.0, 2.55, 1.380, 4.44, 4.4, 4983],\n",
    "            \"cosine\": [1.0, 2.55, 1.380, 4.44, 4.4, 4983],\n",
    "            ...\n",
    "        },\n",
    "        \"numberbatch\": {\n",
    "            \"euclidean\": [1.0, 2.55, 1.380, 4.44, 4.4, 4983],\n",
    "            \"cosine\": [1.0, 2.55, 1.380, 4.44, 4.4, 4983],\n",
    "            ...        \n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    def show_boxplots(ratios: dict[str, dict[str, list[float]]]):\n",
    "        distances: list[dict[str, list[float]]] = list(ratios.values())\n",
    "        # Get all the values per distance type for every embedding\n",
    "        to_plot = {}\n",
    "        for i in range(0, len(distances)):\n",
    "            for embedding_name, distances2 in ratios.items():\n",
    "                for distance_type, values in distances2.items():\n",
    "                    print(embedding_name, distance_type, values)\n",
    "                    if to_plot.get(distance_type) is None:\n",
    "                        to_plot[distance_type] = {}\n",
    "                    to_plot[distance_type][embedding_name] = values\n",
    "        \n",
    "        # For every distance metric, plot boxplots for all embeddings\n",
    "        for distance_type, embedding_data in to_plot.items():\n",
    "            fig, axs = plt.subplots(figsize=(10, 8))\n",
    "            boxplots_data = []\n",
    "            labels = []\n",
    "            for embedding_name, values in embedding_data.items():\n",
    "                boxplots_data.append(values)\n",
    "                labels.append(embedding_name)\n",
    "            axs.boxplot(boxplots_data)\n",
    "            axs.set_xticklabels(labels)\n",
    "            axs.set_title(distance_type)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    # Dict of embedding name to distance type and its calculated distance ratio\n",
    "    ratios = calculate_distance_ratios()\n",
    "    show_boxplots(ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_embeddings = get_embeddings(embeddings)\n",
    "comp_embeddings = get_comparison_embeddings(orig_embeddings)\n",
    "comp_distances = compare_embeddings(comp_embeddings)\n",
    "compare_distances(comp_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
