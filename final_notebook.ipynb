{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from types import FunctionType\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchtext\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "BASE_PATH: str = os.path.dirname(os.path.abspath(__file__))\n",
    "CLASSIFIERS_PATH: str = BASE_PATH + \"/classifiers/\"\n",
    "DATASET_PATH: str = BASE_PATH + \"/datasets/\"\n",
    "EMBEDDINGS_PATH: str = BASE_PATH + \"/embeddings/\"\n",
    "NUM_EMOTIONS: int = 28\n",
    "EMBED_SIZE: int = 0\n",
    "\n",
    "if not os.path.exists(CLASSIFIERS_PATH):\n",
    "    raise FileNotFoundError(\"Could not find folder for classifier models.\")\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    raise FileNotFoundError(\"Could not find folder with GoEmotion dataset.\")\n",
    "if not os.path.exists(EMBEDDINGS_PATH):\n",
    "    raise FileNotFoundError(\"Could not find folder with word embeddings sets.\")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: Using CPU for Pytorch.\")\n",
    "device: device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# First, let's define our basic BDRNN architecture\n",
    "class BDRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_word_count: int, vectors: torch.Tensor, output_size: int, num_layers: int, dropout: float,\n",
    "                 *args: tuple[any],\n",
    "                 **kwargs: dict[str, any]) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.num_layers = num_layers if num_layers > 1 else 2\n",
    "        self.hidden_size = NUM_EMOTIONS // num_layers\n",
    "\n",
    "        self.embeddings = torch.nn.Embedding.from_pretrained(vectors, padding_idx=EMBED_SIZE)\n",
    "\n",
    "        self.rnn_layers = torch.nn.RNN(input_size=vocab_word_count, hidden_size=self.hidden_size, num_layers=num_layers,\n",
    "                                       bidirectional=True, dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.output_layer = torch.nn.Linear(self.hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_data) -> torch.Tensor:\n",
    "        embedded: torch.Tensor = self.embeddings(input_data)\n",
    "\n",
    "        output: torch.Tensor\n",
    "        hidden: torch.Tensor\n",
    "        output, hidden = self.rnn_layers(embedded)\n",
    "\n",
    "        return self.output_layer(hidden[-1, :])\n",
    "\n",
    "\n",
    "class pandas_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame) -> None:\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index: int) -> (str, str):\n",
    "        return self.df[\"text\"].iloc[index], self.df[\"emotion_ids\"].iloc[index]\n",
    "\n",
    "\n",
    "def get_vectors(embedding: str) -> tuple[dict[str, int], torch.Tensor]:\n",
    "    skip_first_line: bool = False\n",
    "    global EMBED_SIZE  # Sorry\n",
    "    match embedding:\n",
    "        case \"glove\":\n",
    "            embedding_path: str = EMBEDDINGS_PATH + \"glove.840B.300d.txt\"\n",
    "            EMBED_SIZE = 2196018\n",
    "            embedding_components: int = 300\n",
    "        case \"word2vec\":\n",
    "            embedding_path: str = EMBEDDINGS_PATH + \"GoogleNews-vectors-negative300.bin\"\n",
    "            gn_model = KeyedVectors.load_word2vec_format(embedding_path, binary=True)\n",
    "            # Ignoring this for now, too lazy to deserialize\n",
    "        case \"numberbatch\":\n",
    "            embedding_path: str = EMBEDDINGS_PATH + \"numberbatch-19.08-en.txt\"\n",
    "            EMBED_SIZE = 516782\n",
    "            embedding_components: int = 300\n",
    "            skip_first_line = True\n",
    "        case default:\n",
    "            raise RuntimeError(\"Invalid embedding chosen.\")\n",
    "\n",
    "    if not os.path.exists(embedding_path):\n",
    "        raise FileNotFoundError(\"Could not find embedding file: {}\".format(embedding_path))\n",
    "    with (open(embedding_path, encoding=\"utf_8\") as embeddings_file):\n",
    "        word_labels: dict[str, int] = {}\n",
    "        tensor: torch.Tensor = torch.empty((EMBED_SIZE + 1, embedding_components), dtype=torch.float32, device=device)\n",
    "        if skip_first_line:\n",
    "            _ = embeddings_file.readline()\n",
    "        for index, embedding in enumerate(embeddings_file):\n",
    "            embedding_split: list[str] = embedding.rstrip().split(\" \")\n",
    "            word_labels[embedding_split[0]] = index\n",
    "            tensor[index] = torch.tensor([float(val) for val in embedding_split[1:]], dtype=torch.float32,\n",
    "                                         device=device)\n",
    "            if (index + 1) % 100000 == 0:\n",
    "                print(\"Processed {}/{}\".format(index + 1, EMBED_SIZE))\n",
    "        tensor[-1] = torch.zeros(embedding_components, dtype=torch.float32, device=device)\n",
    "        word_labels[\"<PAD>\"] = EMBED_SIZE\n",
    "        tensor.to(device)  # Unneeded?\n",
    "        return word_labels, tensor\n",
    "\n",
    "\n",
    "def tokenize(text: str, labels: dict, tokenizer: FunctionType) -> list[int]:\n",
    "    return [labels[word] if word in labels.keys() else labels[\"something\"] for word in tokenizer(text)]\n",
    "\n",
    "\n",
    "def resolve_emotions(id: str) -> str:\n",
    "    return [emotions[int(emotion)] for emotion in id.split(\",\")]\n",
    "\n",
    "\n",
    "def train(model: BDRNN, batches, num_epochs: int):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    losses = []\n",
    "\n",
    "    model.train()\n",
    "    for epoch_num, epochs in enumerate(range(num_epochs)):\n",
    "        correct: int = 0\n",
    "        total: int = 0\n",
    "        for num_batch, batch in enumerate(batches):\n",
    "            for sentence, emotions in batch:\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                predictions = model(sentence)\n",
    "                # Rounding is naive, we should base this off a confidence threshold\n",
    "                guesses = torch.round(torch.sigmoid(predictions))\n",
    "                if torch.equal(guesses, emotions): correct += 1\n",
    "                total += 1\n",
    "\n",
    "                loss = criterion(predictions, emotions)\n",
    "                losses.append(float(loss))\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "        print(\"Epoch: {} | Loss: {} | Accuracy: {}%\".format(epoch_num + 1, sum(losses) / len(losses), (correct /\n",
    "                                                                                                      total) * 100))\n",
    "\n",
    "\n",
    "def collate(batch: list[tuple[list[int], list[str]]]) -> list[tuple[torch.IntTensor, torch.Tensor]]:\n",
    "    final_batch = []\n",
    "    max_tokens = len(max(batch, key=lambda tuple: len(tuple[0]))[0])\n",
    "    for sentence, emotions in batch:\n",
    "        sentence.extend([EMBED_SIZE] * (max_tokens - len(sentence)))\n",
    "        sentence = torch.IntTensor([int(value) for value in sentence]).to(device)\n",
    "        # There's definitely a way to do a list comprehension here but I'm too stupid to figure it out\n",
    "        _emotions = torch.zeros(NUM_EMOTIONS, dtype=torch.float32, device=device)\n",
    "        emotions = emotions.split(\",\")\n",
    "        for emotion in emotions:\n",
    "            _emotions[int(emotion)] = 1.0\n",
    "        final_batch.append((sentence, _emotions))\n",
    "    return final_batch  # Can we modify in-place instead?\n",
    "\n",
    "def main():\n",
    "    # Now we need to handle our dataset\n",
    "    with open(DATASET_PATH + \"emotions.txt\") as emotions_file:\n",
    "        emotions = [emotion.strip() for emotion in emotions_file]\n",
    "    if len(emotions) != NUM_EMOTIONS or emotions[4] != \"approval\":\n",
    "        raise RuntimeError(\"Failed to load emotion mappings.\")\n",
    "\n",
    "    training_set = pd.read_csv(DATASET_PATH + \"train.tsv\", delimiter=\"\\t\", names=[\"text\", \"emotion_ids\"],\n",
    "                               usecols=[0, 1])\n",
    "    testing_set = pd.read_csv(DATASET_PATH + \"test.tsv\", delimiter=\"\\t\", usecols=[0, 1])\n",
    "    print(training_set.head())\n",
    "    print(testing_set.head())\n",
    "\n",
    "    max_words: int = max(training_set[\"text\"].map(len).max(), testing_set[\"text\"].map(len).max())\n",
    "    input_dim: int = 2 ** math.ceil(math.log2(max_words)) if max_words >= 2 else 2\n",
    "\n",
    "    # Time to do some training!\n",
    "    labels, vectors = get_vectors(\"numberbatch\")\n",
    "    tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")\n",
    "    training_set[\"text\"] = training_set[\"text\"].apply(tokenize, labels=labels, tokenizer=tokenizer)\n",
    "    testing_set[\"text\"] = testing_set[\"text\"].apply(tokenize, labels=labels, tokenizer=tokenizer)\n",
    "    print(training_set.head())\n",
    "    print(testing_set.head())\n",
    "    numberbatch_model = BDRNN(vectors.shape[1], vectors, NUM_EMOTIONS, 4, 0.5).to(device)\n",
    "    train_dataset = pandas_dataset(training_set)\n",
    "    test_dataset = pandas_dataset(testing_set)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "    print('Created `training dataloader` with %d batches!' % len(train_dataloader))\n",
    "    print('Created `testing dataloader` with %d batches!' % len(test_dataloader))\n",
    "    train(numberbatch_model, train_dataloader, 10)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download link for word2vec: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n",
    "# Download link for Glove: https://huggingface.co/stanfordnlp/glove/resolve/main/glove.840B.300d.zip\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "from sklearn.manifold import TSNE\n",
    "from torch import Tensor, cat\n",
    "from torch.cuda import is_available as cuda_is_available\n",
    "from train import get_vectors\n",
    "\n",
    "\n",
    "# EDITABLE VARIABLES\n",
    "embeddings = [\"numberbatch\", \"glove\"] # Embeddings definition. Add word2vec once deserialization is done.\n",
    "\n",
    "# Word comparison groups. Format: [base_word, similar_word_1, similar_word_2]\n",
    "word_comparison_groups = [\n",
    "    [\"tire\", \"tired\", \"tyre\"],\n",
    "]\n",
    "\n",
    "# Add a new distance function here if you want.\n",
    "def calculate_distances(base_word: ndarray[float], similar_word_1: ndarray[float], similar_word_2: ndarray[float]) -> dict[str, list[float]]:\n",
    "    return {\n",
    "        \"euclidean\": [euclidean_distance(similar_word_1, base_word), euclidean_distance(similar_word_2, base_word)],\n",
    "        \"cosine\": [cosine_similarity(similar_word_1, base_word), cosine_similarity(similar_word_2, base_word)],\n",
    "        \"manhattan\": [manhattan_distance(similar_word_1, base_word), manhattan_distance(similar_word_2, base_word)],\n",
    "    }\n",
    "\n",
    "\n",
    "# We should look at comparing vectors in different embeddings and see how well ambigious words center around common\n",
    "# synonyms for each meaning. We could probably do some sort of visualization for this as well.\n",
    "\n",
    "def euclidean_distance(vector1: Tensor, vector2: Tensor) -> float:\n",
    "    return np.linalg.norm(vector1 - vector2)\n",
    "\n",
    "def cosine_similarity(vector1: Tensor, vector2: Tensor) -> float:\n",
    "    return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "def manhattan_distance(v1, v2):\n",
    "    return np.sum(np.abs(v1 - v2))\n",
    "\n",
    "def convert_tensors_to_numpy(embeddings_list: list[Tensor]) -> ndarray[float]:\n",
    "    if cuda_is_available():\n",
    "        numpy_vectors = np.array([vector.cpu().numpy() for vector in embeddings_list])\n",
    "    else:\n",
    "        numpy_vectors = np.array([vector.numpy() for vector in embeddings_list])\n",
    "    return numpy_vectors\n",
    "\n",
    "def reduce_embeddings(numpy_vectors: ndarray[float]) -> ndarray[float]:\n",
    "    tsne = TSNE(n_components=3, random_state=0, perplexity=2, init='pca', n_iter=6000)\n",
    "    return tsne.fit_transform(numpy_vectors)\n",
    "\n",
    "def get_word_vectors_to_compare(numpy_vectors: ndarray[float]) -> tuple[ndarray[float], ndarray[float], ndarray[float]]:\n",
    "    return numpy_vectors[0], numpy_vectors[1], numpy_vectors[2]\n",
    "\n",
    "def show_plot_comparison(reduced_vectors: ndarray[float], word_list: list[str], norm_distances: dict[str, list[float]]):\n",
    "    for distance_type, calculated_distances in norm_distances.items():\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "        ax.scatter(reduced_vectors[0][0], reduced_vectors[0][1], reduced_vectors[0][2], c='b', label=word_list[0])\n",
    "        ax.scatter(reduced_vectors[1][0], reduced_vectors[1][1], reduced_vectors[1][2], c='g', label=word_list[1])\n",
    "        ax.scatter(reduced_vectors[2][0], reduced_vectors[2][1], reduced_vectors[2][2], c='r', label=word_list[2])\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "        print(\"{} distance between 'tire' and 'tired' is {}\\nDistance between 'tire' and 'tyre' is {}\".format(\n",
    "            distance_type, calculated_distances[0], calculated_distances[1]))\n",
    "\n",
    "def get_distances(embeddings_list: list[Tensor]) -> dict[str, list[float]]:\n",
    "    numpy_vectors = convert_tensors_to_numpy(embeddings_list)\n",
    "    base_word, similar_word1, similar_word2 = get_word_vectors_to_compare(numpy_vectors)\n",
    "    return calculate_distances(base_word, similar_word1, similar_word2)\n",
    "\n",
    "def get_reduced_vectors(embeddings_list: list[Tensor]) -> ndarray[float, float]:\n",
    "        numpy_vectors = convert_tensors_to_numpy(embeddings_list)\n",
    "        return reduce_embeddings(numpy_vectors)\n",
    "\n",
    "def visualize_embedding(embeddings_list: list[Tensor], norm_distances: dict[str, list[float]], word_list: list[str]) -> dict[str, list[float]]:\n",
    "    reduced_vectors: ndarray[float, float] = get_reduced_vectors(embeddings_list)\n",
    "    show_plot_comparison(reduced_vectors, word_list, norm_distances)\n",
    "\n",
    "def get_embeddings(embeddings: list[str]) -> dict[str, (dict[str, int], Tensor)]:\n",
    "    if len(embeddings) == 0:\n",
    "        raise ValueError(\"No embeddings were selected to load.\")\n",
    "    \n",
    "    result = {}\n",
    "    for embedding in embeddings:\n",
    "        vocab, vectors = get_vectors(embedding)\n",
    "        result[embedding] = (vocab, vectors)\n",
    "    return result\n",
    "\n",
    "# Bc comparison groups added in sequence, we can query word_comparison_groups by index from where the group appears in the result. \n",
    "# (result[embedding_name][0] for the first word group)\n",
    "def get_comparison_embeddings(embeddings: dict[str, (dict[str, int], Tensor)]) -> dict[str, list[list[Tensor]]]:\n",
    "    # Dictionary of embedding name to list of comparison groups\n",
    "    result: dict[str, list[list[Tensor]]] = {}\n",
    "    for embed_name, (vocab, vectors) in embeddings.items():\n",
    "        comparisons = []\n",
    "        if embed_name not in result:\n",
    "            result[embed_name] = []\n",
    "        for idx, group in enumerate(word_comparison_groups):\n",
    "            comparisons = []\n",
    "            for word in group:\n",
    "                comparisons.append(vectors[vocab[word]])\n",
    "            result[embed_name].append(comparisons)\n",
    "    return result\n",
    "\n",
    "\"\"\"Returns a dictionary of embedding names to a list of dictionaries of distance types to their calculated distances\"\"\"\n",
    "def compare_embeddings(comparison_embeddings: dict[str, list[list[Tensor]]]) -> dict[str, list[dict[str, list[float]]]]:\n",
    "    result = {}\n",
    "    for embedding, word_groups in comparison_embeddings.items():\n",
    "        for idx, group_vectors in enumerate(word_groups):\n",
    "            if result.get(embedding) is None:\n",
    "                result[embedding] = []\n",
    "            distances: dict[str, list[float]] = get_distances(group_vectors)\n",
    "            result[embedding].append(distances)\n",
    "    return result\n",
    "\n",
    "def compare_distances(embedding_distances: dict[str, list[dict[str, list[float]]]]):\n",
    "    \"\"\"Returns a dictionary of embedding names to a dictionary of distance types to their calculated distance ratios.\"\"\"\n",
    "    def calculate_distance_ratios():\n",
    "        # Dict of embedding name to distance type and all of that distance type's calculated distance ratios\n",
    "        ratios: dict[str, dict[str, list[float]]] = {}\n",
    "        # For every distance type for embeddings\n",
    "        for embedding_name, distances in embedding_distances.items():\n",
    "            ratios[embedding_name] = {}\n",
    "            for distance in distances:\n",
    "                for distance_type, values in distance.items():\n",
    "                    if ratios[embedding_name].get(distance_type) is None:\n",
    "                        ratios[embedding_name][distance_type] = []\n",
    "                    ratio = (max(values[0], values[1]) / min(values[0], values[1]))\n",
    "                    ratios[embedding_name][distance_type].append(ratio)\n",
    "        return ratios\n",
    "\n",
    "    \"\"\"\n",
    "    Plots boxplots of the distance ratios for each embedding and distance type.\n",
    "    Args: ratios: dict[str, dict[str, list[float]]] - Dictionary of embedding names to a dictionary of distance types to a list of all their calculated distance ratios.\n",
    "    \n",
    "    Ex:\n",
    "    ratios = {\n",
    "        \"glove\": {\n",
    "            \"euclidean\": [1.0, 2.55, 1.380, 4.44, 4.4, 4983],\n",
    "            \"cosine\": [1.0, 2.55, 1.380, 4.44, 4.4, 4983],\n",
    "            ...\n",
    "        },\n",
    "        \"numberbatch\": {\n",
    "            \"euclidean\": [1.0, 2.55, 1.380, 4.44, 4.4, 4983],\n",
    "            \"cosine\": [1.0, 2.55, 1.380, 4.44, 4.4, 4983],\n",
    "            ...        \n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    def show_boxplots(ratios: dict[str, dict[str, list[float]]]):\n",
    "        distances: list[dict[str, list[float]]] = list(ratios.values())\n",
    "        # Get all the values per distance type for every embedding\n",
    "        to_plot = {}\n",
    "        for i in range(0, len(distances)):\n",
    "            for embedding_name, distances2 in ratios.items():\n",
    "                for distance_type, values in distances2.items():\n",
    "                    print(embedding_name, distance_type, values)\n",
    "                    if to_plot.get(distance_type) is None:\n",
    "                        to_plot[distance_type] = {}\n",
    "                    to_plot[distance_type][embedding_name] = values\n",
    "                    \n",
    "        for distance_type, embedding_data in to_plot.items():\n",
    "            fig, axs = plt.subplots(figsize=(10, 8))\n",
    "            boxplots_data = []\n",
    "            labels = []\n",
    "            for embedding_name, values in embedding_data.items():\n",
    "                boxplots_data.append(values)\n",
    "                labels.append(embedding_name)\n",
    "            axs.boxplot(boxplots_data)\n",
    "            axs.set_xticklabels(labels)\n",
    "            axs.set_title(distance_type)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    # Dict of embedding name to distance type and its calculated distance ratio\n",
    "    ratios = calculate_distance_ratios()\n",
    "    show_boxplots(ratios)\n",
    "\n",
    "orig_embeddings = get_embeddings(embeddings)\n",
    "comp_embeddings = get_comparison_embeddings(orig_embeddings)\n",
    "comp_distances = compare_embeddings(comp_embeddings)\n",
    "compare_distances(comp_distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
