{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "\n",
    "# Created by Bryce Shurts on January 29th, 2024\n",
    "# Purpose: Create classifer models\n",
    "# Note: When I made this I did not realize torch.Tensor (and the other constructors like IntTensor) were deprecated in\n",
    "# favor of torch.tensor (note the case). torch.Tensor is technically legacy and unsupported but it works so I'm not\n",
    "# going to bother refactoring everything over to torch.tensor & torch.empty.\n",
    "import math\n",
    "import os\n",
    "from types import FunctionType\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchtext\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "BASE_PATH: str = os.path.dirname(os.path.abspath(__file__))\n",
    "CLASSIFIERS_PATH: str = BASE_PATH + \"/classifiers/\"\n",
    "DATASET_PATH: str = BASE_PATH + \"/datasets/\"\n",
    "EMBEDDINGS_PATH: str = BASE_PATH + \"/embeddings/\"\n",
    "NUM_EMOTIONS: int = 28\n",
    "EMBED_SIZE: int = 0\n",
    "\n",
    "if not os.path.exists(CLASSIFIERS_PATH):\n",
    "    raise FileNotFoundError(\"Could not find folder for classifier models.\")\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    raise FileNotFoundError(\"Could not find folder with GoEmotion dataset.\")\n",
    "if not os.path.exists(EMBEDDINGS_PATH):\n",
    "    raise FileNotFoundError(\"Could not find folder with word embeddings sets.\")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: Using CPU for Pytorch.\")\n",
    "device: device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# First, let's define our basic BDRNN architecture\n",
    "class BDRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_word_count: int, vectors: torch.Tensor, output_size: int, num_layers: int, dropout: float,\n",
    "                 *args: tuple[any],\n",
    "                 **kwargs: dict[str, any]) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.num_layers = num_layers if num_layers > 1 else 2\n",
    "        self.hidden_size = NUM_EMOTIONS // num_layers\n",
    "\n",
    "        self.embeddings = torch.nn.Embedding.from_pretrained(vectors, padding_idx=EMBED_SIZE)\n",
    "\n",
    "        self.rnn_layers = torch.nn.RNN(input_size=vocab_word_count, hidden_size=self.hidden_size, num_layers=num_layers,\n",
    "                                       bidirectional=True, dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.output_layer = torch.nn.Linear(self.hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_data) -> torch.Tensor:\n",
    "        embedded: torch.Tensor = self.embeddings(input_data)\n",
    "\n",
    "        output: torch.Tensor\n",
    "        hidden: torch.Tensor\n",
    "        output, hidden = self.rnn_layers(embedded)\n",
    "\n",
    "        return self.output_layer(hidden[-1, :])\n",
    "\n",
    "\n",
    "class pandas_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame) -> None:\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index: int) -> (str, str):\n",
    "        return self.df[\"text\"].iloc[index], self.df[\"emotion_ids\"].iloc[index]\n",
    "\n",
    "\n",
    "def get_vectors(embedding: str) -> tuple[dict[str, int], torch.Tensor]:\n",
    "    skip_first_line: bool = False\n",
    "    global EMBED_SIZE  # Sorry\n",
    "    match embedding:\n",
    "        case \"glove\":\n",
    "            embedding_path: str = EMBEDDINGS_PATH + \"glove.840B.300d.txt\"\n",
    "            EMBED_SIZE = 2196018\n",
    "            embedding_components: int = 300\n",
    "        case \"word2vec\":\n",
    "            embedding_path: str = EMBEDDINGS_PATH + \"GoogleNews-vectors-negative300.bin\"\n",
    "            gn_model = KeyedVectors.load_word2vec_format(embedding_path, binary=True)\n",
    "            # Ignoring this for now, too lazy to deserialize\n",
    "        case \"numberbatch\":\n",
    "            embedding_path: str = EMBEDDINGS_PATH + \"numberbatch-19.08-en.txt\"\n",
    "            EMBED_SIZE = 516782\n",
    "            embedding_components: int = 300\n",
    "            skip_first_line = True\n",
    "        case default:\n",
    "            raise RuntimeError(\"Invalid embedding chosen.\")\n",
    "\n",
    "    if not os.path.exists(embedding_path):\n",
    "        raise FileNotFoundError(\"Could not find embedding file: {}\".format(embedding_path))\n",
    "    with (open(embedding_path, encoding=\"utf_8\") as embeddings_file):\n",
    "        word_labels: dict[str, int] = {}\n",
    "        tensor: torch.Tensor = torch.empty((EMBED_SIZE + 1, embedding_components), dtype=torch.float32, device=device)\n",
    "        if skip_first_line:\n",
    "            _ = embeddings_file.readline()\n",
    "        for index, embedding in enumerate(embeddings_file):\n",
    "            embedding_split: list[str] = embedding.rstrip().split(\" \")\n",
    "            word_labels[embedding_split[0]] = index\n",
    "            tensor[index] = torch.tensor([float(val) for val in embedding_split[1:]], dtype=torch.float32,\n",
    "                                         device=device)\n",
    "            if (index + 1) % 100000 == 0:\n",
    "                print(\"Processed {}/{}\".format(index + 1, EMBED_SIZE))\n",
    "        tensor[-1] = torch.zeros(embedding_components, dtype=torch.float32, device=device)\n",
    "        word_labels[\"<PAD>\"] = EMBED_SIZE\n",
    "        tensor.to(device)  # Unneeded?\n",
    "        return word_labels, tensor\n",
    "\n",
    "\n",
    "def tokenize(text: str, labels: dict, tokenizer: FunctionType) -> list[int]:\n",
    "    return [labels[word] if word in labels.keys() else labels[\"something\"] for word in tokenizer(text)]\n",
    "\n",
    "\n",
    "def resolve_emotions(id: str) -> str:\n",
    "    return [emotions[int(emotion)] for emotion in id.split(\",\")]\n",
    "\n",
    "\n",
    "def train(model: BDRNN, batches, num_epochs: int):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    losses = []\n",
    "\n",
    "    model.train()\n",
    "    for epoch_num, epochs in enumerate(range(num_epochs)):\n",
    "        correct: int = 0\n",
    "        total: int = 0\n",
    "        for num_batch, batch in enumerate(batches):\n",
    "            for sentence, emotions in batch:\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                predictions = model(sentence)\n",
    "                # Rounding is naive, we should base this off a confidence threshold\n",
    "                guesses = torch.round(torch.sigmoid(predictions))\n",
    "                if torch.equal(guesses, emotions): correct += 1\n",
    "                total += 1\n",
    "\n",
    "                loss = criterion(predictions, emotions)\n",
    "                losses.append(float(loss))\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "        print(\"Epoch: {} | Loss: {} | Accuracy: {}%\".format(epoch_num + 1, sum(losses) / len(losses), (correct /\n",
    "                                                                                                      total) * 100))\n",
    "\n",
    "\n",
    "def collate(batch: list[tuple[list[int], list[str]]]) -> list[tuple[torch.IntTensor, torch.Tensor]]:\n",
    "    final_batch = []\n",
    "    max_tokens = len(max(batch, key=lambda tuple: len(tuple[0]))[0])\n",
    "    for sentence, emotions in batch:\n",
    "        sentence.extend([EMBED_SIZE] * (max_tokens - len(sentence)))\n",
    "        sentence = torch.IntTensor([int(value) for value in sentence]).to(device)\n",
    "        # There's definitely a way to do a list comprehension here but I'm too stupid to figure it out\n",
    "        _emotions = torch.zeros(NUM_EMOTIONS, dtype=torch.float32, device=device)\n",
    "        emotions = emotions.split(\",\")\n",
    "        for emotion in emotions:\n",
    "            _emotions[int(emotion)] = 1.0\n",
    "        final_batch.append((sentence, _emotions))\n",
    "    return final_batch  # Can we modify in-place instead?\n",
    "\n",
    "def main():\n",
    "    # Now we need to handle our dataset\n",
    "    with open(DATASET_PATH + \"emotions.txt\") as emotions_file:\n",
    "        emotions = [emotion.strip() for emotion in emotions_file]\n",
    "    if len(emotions) != NUM_EMOTIONS or emotions[4] != \"approval\":\n",
    "        raise RuntimeError(\"Failed to load emotion mappings.\")\n",
    "\n",
    "    training_set = pd.read_csv(DATASET_PATH + \"train.tsv\", delimiter=\"\\t\", names=[\"text\", \"emotion_ids\"],\n",
    "                               usecols=[0, 1])\n",
    "    testing_set = pd.read_csv(DATASET_PATH + \"test.tsv\", delimiter=\"\\t\", usecols=[0, 1])\n",
    "    print(training_set.head())\n",
    "    print(testing_set.head())\n",
    "\n",
    "    max_words: int = max(training_set[\"text\"].map(len).max(), testing_set[\"text\"].map(len).max())\n",
    "    input_dim: int = 2 ** math.ceil(math.log2(max_words)) if max_words >= 2 else 2\n",
    "\n",
    "    # Time to do some training!\n",
    "    labels, vectors = get_vectors(\"numberbatch\")\n",
    "    tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")\n",
    "    training_set[\"text\"] = training_set[\"text\"].apply(tokenize, labels=labels, tokenizer=tokenizer)\n",
    "    testing_set[\"text\"] = testing_set[\"text\"].apply(tokenize, labels=labels, tokenizer=tokenizer)\n",
    "    print(training_set.head())\n",
    "    print(testing_set.head())\n",
    "    numberbatch_model = BDRNN(vectors.shape[1], vectors, NUM_EMOTIONS, 4, 0.5).to(device)\n",
    "    train_dataset = pandas_dataset(training_set)\n",
    "    test_dataset = pandas_dataset(testing_set)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "    print('Created `training dataloader` with %d batches!' % len(train_dataloader))\n",
    "    print('Created `testing dataloader` with %d batches!' % len(test_dataloader))\n",
    "    train(numberbatch_model, train_dataloader, 10)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
