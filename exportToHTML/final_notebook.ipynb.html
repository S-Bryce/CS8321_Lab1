<html>
<head>
<title>final_notebook.ipynb</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #bcbec4;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
final_notebook.ipynb</font>
</center></td></tr></table>
<pre><span class="s0">{ 
 &quot;cells&quot;: [ 
  { 
   &quot;cell_type&quot;: &quot;markdown&quot;, 
   &quot;metadata&quot;: {}, 
   &quot;source&quot;: [ 
    &quot;Authors: Michael Amberg, Wali Chaudhary, Bryce Shurts&quot; 
   ] 
  }, 
  { 
   &quot;cell_type&quot;: &quot;markdown&quot;, 
   &quot;source&quot;: [ 
    &quot;# Note: This project was coded in python &amp; transferred to a IPYNB for final submission as an HTML file.&quot; 
   ], 
   &quot;metadata&quot;: { 
    &quot;collapsed&quot;: false 
   } 
  }, 
  { 
   &quot;cell_type&quot;: &quot;markdown&quot;, 
   &quot;source&quot;: [ 
    &quot;RUBRIC\n&quot;, 
    &quot;\n&quot;, 
    &quot;[2 Points] Present an overview for what type of bias you will be investigating and why the particular investigation you will be doing is relevant. You might consider asking questions like: Why is it important to find this kind of bias in machine learning models? Why will the type of investigation I am performing be relevant to other researchers or practitioners? \n&quot;, 
    &quot;\n&quot;, 
    &quot;[2 Points] Present one or more research questions that you will be answering and explain the methods that you will employ to answer these research questions. Present a hypothesis as part of your research questions. \n&quot;, 
    &quot;\n&quot;, 
    &quot;[2 Points] As part of your assignment, you will choose a methodology that involves comparing two (or more) techniques to one another. Discuss how you will measure a difference between the two techniques. That is, if you are measuring the difference statistically, what test will you use and why is it appropriate? Are there any limitations to performing this test that you should be aware of? \n&quot;, 
    &quot;\n&quot;, 
    &quot;[4 Points] Carryout your analysis and model training. Explain your steps in as much detail so that the instructor can understand your code. \n&quot;, 
    &quot;\n&quot;, 
    &quot;[4 Points] Present results from your analysis and provide evidence from the results that support or refute your hypothesis. Write a conclusion based upon the various analyses you performed. Be sure to reference your research questions systematically in your conclusion. With your analysis complete, are there any additional research questions or limitations to your conclusions?\n&quot;, 
    &quot;\n&quot;, 
    &quot;[1 Points] Identify two conferences or journals that would be interested in the results of your analysis.  \n&quot;, 
    &quot;If using code from another author (not your own), you will be graded on the clarity of explanatory comments you add to the code. \n&quot; 
   ], 
   &quot;metadata&quot;: { 
    &quot;collapsed&quot;: false 
   } 
  }, 
  { 
   &quot;cell_type&quot;: &quot;markdown&quot;, 
   &quot;source&quot;: [ 
    &quot;# Analysis overview\n&quot;, 
    &quot;While much of the lab and lessons focus on types of semantic bias embedded into vectors (racial, gender, etc.), we seek to evaluate a more definitionally-focused bias. That is, we seek to investigate the sense bias of the single-sense word embeddings Glove &amp; Numberbatch, and evaluate their ability to avoid biasing toward certain definitions of a set of given words.\n&quot;, 
    &quot;Our inspiration for this comes from reasoning agents, such as logic tensor networks (LTNs), that might rely upon semantic vectors for sub-symbolic reasoning and extrapolation of information outside the knowledge of its knowledge base. Consider the following sentence: “I really enjoyed that club today.” Is the subject talking about a golf club, a club sandwich, or an organization? Using our reasoning abilities as human beings, we can infer from the sentence structure that the usage of “club” probably refers to an object like a sandwich or a golf club, rather than an organizational body (in which case, “the club meeting”, “enjoyed trying out that club”, etc. would be a more common phrasing), but an AI does not have this ability: it must rely on the embeddings it has been given.\n&quot;, 
    &quot;In a perfectly unbiased system, i.e., one that has the vector of “club” perfectly equidistance in all components for all possible disambiguations of the word, we would not expect the agent/system to infer a bias towards one definition or another, and for there to be a good chance that it determines an accurate conclusion when processing the sentence against some given query (e.g., “Was the club tasty?”). In a biased case, however, the network may have difficult resolving the information on how the word is represented versus what the word meant in proper context: in the worst case it will only have information from a single one of the possible definitions, which would cause the reasonability score of a query to drop even more sharply than in the merely biased case. Therefore, we believe it important to evaluate this kind of bias for the benefit of researchers attempting to utilize pre-trained word embeddings to add additional information to their networks where systemic definitional bias in a single-sense embedding could impair performance.\n&quot; 
   ], 
   &quot;metadata&quot;: { 
    &quot;collapsed&quot;: false 
   } 
  }, 
  { 
   &quot;cell_type&quot;: &quot;markdown&quot;, 
   &quot;source&quot;: [ 
    &quot;# Research questions &amp; hypothesis\n&quot;, 
    &quot;Before we begin any research and experimentation at all, we must first define what we are trying to figure out, the questions that define this, and provide an expectation that we will be seeing to find evidence in support or refutation of. Put simply, we wish to evaluate whether or not the semantic de-biasing that Numberbatch is well-known for has extended into its performance w.r.t word senses and its ability to evenly represent multiple word senses within a single vector. More specifically, we ask the following:\n&quot;, 
    &quot;1.\tHas Numberbatch’s efforts to improve upon previous word embedding performance (multi-lingual, semantic debiasing, etc.) improved its ability to provide well-centered definitions for ambiguous words?\n&quot;, 
    &quot;2.\tIf Numberbatch does have an improved ability to equally represent multiple senses, in what cases is this? Is it universally better, or is it only better when specific measures are applied (e.g., is only the cosine similarity metric improved considerably? Is it better in all respects to the Minkowski distance regardless of p value? Is it always worse than some other word embedding?)\n&quot;, 
    &quot;These questions, then, inform on the reasonability and accuracy of our hypothesis: Due to Numberbatch’s multi-lingual nature and debiasing efforts, we believe that it will exhibit superior performance vis-à-vis having semantic vectors that are near-equidistant to disambiguated definitional senses.\n&quot; 
   ], 
   &quot;metadata&quot;: { 
    &quot;collapsed&quot;: false 
   } 
  }, 
  { 
   &quot;cell_type&quot;: &quot;markdown&quot;, 
   &quot;source&quot;: [ 
    &quot;# Measurement strategy\n&quot;, 
    &quot;To answer these questions and attempt to find an answer for our hypothesis, we consider two separate metrics: the distance between vectors, and how similar any 2 given vectors are. For measuring distance, we use the Minkowski distance formula with 2 sperate p values, p=1 (the Manhattan distance) &amp; p=2 (the Euclidian distance). We believe that the Manhattan distance will provide better results given the high dimensionality space the embeddings operate in, but we will also include the Euclidian distance for completion’s sake and for cross-comparison against the Manhattan distance findings. For comparison vector similarity, we instead turn to the cosine similarity formula, so that we might evaluate how similar any two words, or datapoints, are to each other.\n&quot;, 
    &quot;Within any given measurement method, we will compare each of our 2 embeddings, utilizing a set of words that we have hand-picked for their ambiguity, alongside disambiguating synonyms for the potential definitions. From there, the process is as such:\n&quot;, 
    &quot;1.\tExample words: light (ambiguous word), illumination (disambiguated definition), &amp; lightweight (disambiguated definition)\n&quot;, 
    &quot;2.\tTake the Manhattan distance, Euclidian distance, and Cosine similarity of light &amp; illumination, as well as light &amp; lightweight\n&quot;, 
    &quot;3.\tCompute the ratio of the distance by dividing the larger number by the smaller number\n&quot;, 
    &quot;4.\tRepeat this process for the other word embedding\n&quot;, 
    &quot;5.\tRepeat for all sets of words provided (25 total)\n&quot;, 
    &quot;6.\tBoxplot these ratios on a per-metric basis\n&quot;, 
    &quot;This will allow us to compare several kinds of potential metrics to mitigate potential biases that may result from the representation of distance and/or similarity in just one metric (the Euclidian distance, for instance, may overly equalize the high dimensionality data). By having 3 metrics, we provide the opportunity for there to be a quorum: we expect that in the case of one set of word embeddings performing better than the others w.r.t how well-balanced the ambiguous words are represented, then we should see it outperform the other word embeddings in at least 2 of the 3 metrics.\n&quot; 
   ], 
   &quot;metadata&quot;: { 
    &quot;collapsed&quot;: false 
   } 
  }, 
  { 
   &quot;cell_type&quot;: &quot;code&quot;, 
   &quot;execution_count&quot;: 3, 
   &quot;metadata&quot;: { 
    &quot;ExecuteTime&quot;: { 
     &quot;end_time&quot;: &quot;2024-02-13T05:13:11.634316500Z&quot;, 
     &quot;start_time&quot;: &quot;2024-02-13T05:13:08.109316100Z&quot; 
    } 
   }, 
   &quot;outputs&quot;: [ 
    { 
     &quot;ename&quot;: &quot;FileNotFoundError&quot;, 
     &quot;evalue&quot;: &quot;Could not find folder for classifier models.&quot;, 
     &quot;output_type&quot;: &quot;error&quot;, 
     &quot;traceback&quot;: [ 
      &quot;\u001B[1;31m---------------------------------------------------------------------------\u001B[0m&quot;, 
      &quot;\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)&quot;, 
      &quot;Cell \u001B[1;32mIn[3], line 21\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# Check if our key directories exist\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(CLASSIFIERS_PATH):\n\u001B[1;32m---&gt; 21\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124m\&quot;\u001B[39m\u001B[38;5;124mCould not find folder for classifier models.\u001B[39m\u001B[38;5;124m\&quot;\u001B[39m)\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(DATASET_PATH):\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124m\&quot;\u001B[39m\u001B[38;5;124mCould not find folder with GoEmotion dataset.\u001B[39m\u001B[38;5;124m\&quot;\u001B[39m)\n&quot;, 
      &quot;\u001B[1;31mFileNotFoundError\u001B[0m: Could not find folder for classifier models.&quot; 
     ] 
    } 
   ], 
   &quot;source&quot;: [ 
    &quot;import math\n&quot;, 
    &quot;import os\n&quot;, 
    &quot;from types import FunctionType\n&quot;, 
    &quot;\n&quot;, 
    &quot;import pandas as pd\n&quot;, 
    &quot;import torch\n&quot;, 
    &quot;import torchtext\n&quot;, 
    &quot;from gensim.models import KeyedVectors\n&quot;, 
    &quot;\n&quot;, 
    &quot;# BASE_PATH: str = os.path.dirname(os.path.abspath(__file__))\n&quot;, 
    &quot;BASE_PATH: str = \&quot;/home/paperspace/Desktop/8321-Mach-Lrng-Neural-Ntwrks/Lab1/CS8321_Lab1\&quot; # REPLACE THIS LINE FOR YOUR LOCAL\n&quot;, 
    &quot;\n&quot;, 
    &quot;CLASSIFIERS_PATH: str = BASE_PATH + \&quot;/classifiers/\&quot;\n&quot;, 
    &quot;DATASET_PATH: str = BASE_PATH + \&quot;/datasets/\&quot;\n&quot;, 
    &quot;EMBEDDINGS_PATH: str = BASE_PATH + \&quot;/embeddings/\&quot;\n&quot;, 
    &quot;NUM_EMOTIONS: int = 28\n&quot;, 
    &quot;EMBED_SIZE: int = 0\n&quot;, 
    &quot;\n&quot;, 
    &quot;# Check if our key directories exist\n&quot;, 
    &quot;if not os.path.exists(CLASSIFIERS_PATH):\n&quot;, 
    &quot;    raise FileNotFoundError(\&quot;Could not find folder for classifier models.\&quot;)\n&quot;, 
    &quot;if not os.path.exists(DATASET_PATH):\n&quot;, 
    &quot;    raise FileNotFoundError(\&quot;Could not find folder with GoEmotion dataset.\&quot;)\n&quot;, 
    &quot;if not os.path.exists(EMBEDDINGS_PATH):\n&quot;, 
    &quot;    raise FileNotFoundError(\&quot;Could not find folder with word embeddings sets.\&quot;)\n&quot;, 
    &quot;\n&quot;, 
    &quot;# Is the cuda GPU available?\n&quot;, 
    &quot;if not torch.cuda.is_available():\n&quot;, 
    &quot;    print(\&quot;Warning: Using CPU for Pytorch.\&quot;)\n&quot;, 
    &quot;device: device = torch.device(\&quot;cuda\&quot;) if torch.cuda.is_available() else torch.device(\&quot;cpu\&quot;)&quot; 
   ] 
  }, 
  { 
   &quot;cell_type&quot;: &quot;code&quot;, 
   &quot;execution_count&quot;: 2, 
   &quot;metadata&quot;: {}, 
   &quot;outputs&quot;: [], 
   &quot;source&quot;: [ 
    &quot;class pandas_dataset(torch.utils.data.Dataset):\n&quot;, 
    &quot;    def __init__(self, df: pd.DataFrame) -&gt; None:\n&quot;, 
    &quot;        self.df = df\n&quot;, 
    &quot;\n&quot;, 
    &quot;    def __len__(self) -&gt; int:\n&quot;, 
    &quot;        return self.df.shape[0]\n&quot;, 
    &quot;\n&quot;, 
    &quot;    def __getitem__(self, index: int) -&gt; (str, str):\n&quot;, 
    &quot;        return self.df[\&quot;text\&quot;].iloc[index], self.df[\&quot;emotion_ids\&quot;].iloc[index]&quot; 
   ] 
  }, 
  { 
   &quot;cell_type&quot;: &quot;code&quot;, 
   &quot;execution_count&quot;: 7, 
   &quot;metadata&quot;: {}, 
   &quot;outputs&quot;: [ 
    { 
     &quot;name&quot;: &quot;stdout&quot;, 
     &quot;output_type&quot;: &quot;stream&quot;, 
     &quot;text&quot;: [ 
      &quot;Processed 100000/3000000\n&quot;, 
      &quot;Processed 200000/3000000\n&quot;, 
      &quot;Processed 300000/3000000\n&quot;, 
      &quot;Processed 400000/3000000\n&quot;, 
      &quot;Processed 500000/3000000\n&quot;, 
      &quot;Processed 600000/3000000\n&quot;, 
      &quot;Processed 700000/3000000\n&quot;, 
      &quot;Processed 800000/3000000\n&quot;, 
      &quot;Processed 900000/3000000\n&quot;, 
      &quot;Processed 1000000/3000000\n&quot;, 
      &quot;Processed 1100000/3000000\n&quot;, 
      &quot;Processed 1200000/3000000\n&quot;, 
      &quot;Processed 1300000/3000000\n&quot;, 
      &quot;Processed 1400000/3000000\n&quot;, 
      &quot;Processed 1500000/3000000\n&quot;, 
      &quot;Processed 1600000/3000000\n&quot;, 
      &quot;Processed 1700000/3000000\n&quot;, 
      &quot;Processed 1800000/3000000\n&quot;, 
      &quot;Processed 1900000/3000000\n&quot;, 
      &quot;Processed 2000000/3000000\n&quot;, 
      &quot;Processed 2100000/3000000\n&quot;, 
      &quot;Processed 2200000/3000000\n&quot;, 
      &quot;Processed 2300000/3000000\n&quot;, 
      &quot;Processed 2400000/3000000\n&quot;, 
      &quot;Processed 2500000/3000000\n&quot;, 
      &quot;Processed 2600000/3000000\n&quot;, 
      &quot;Processed 2700000/3000000\n&quot;, 
      &quot;Processed 2800000/3000000\n&quot;, 
      &quot;Processed 2900000/3000000\n&quot;, 
      &quot;Processed 3000000/3000000\n&quot; 
     ] 
    }, 
    { 
     &quot;data&quot;: { 
      &quot;text/plain&quot;: [ 
       &quot;({'&lt;/s&gt;': 0,\n&quot;, 
       &quot;  'in': 1,\n&quot;, 
       &quot;  'for': 2,\n&quot;, 
       &quot;  'that': 3,\n&quot;, 
       &quot;  'is': 4,\n&quot;, 
       &quot;  'on': 5,\n&quot;, 
       &quot;  '##': 6,\n&quot;, 
       &quot;  'The': 7,\n&quot;, 
       &quot;  'with': 8,\n&quot;, 
       &quot;  'said': 9,\n&quot;, 
       &quot;  'was': 10,\n&quot;, 
       &quot;  'the': 11,\n&quot;, 
       &quot;  'at': 12,\n&quot;, 
       &quot;  'not': 13,\n&quot;, 
       &quot;  'as': 14,\n&quot;, 
       &quot;  'it': 15,\n&quot;, 
       &quot;  'be': 16,\n&quot;, 
       &quot;  'from': 17,\n&quot;, 
       &quot;  'by': 18,\n&quot;, 
       &quot;  'are': 19,\n&quot;, 
       &quot;  'I': 20,\n&quot;, 
       &quot;  'have': 21,\n&quot;, 
       &quot;  'he': 22,\n&quot;, 
       &quot;  'will': 23,\n&quot;, 
       &quot;  'has': 24,\n&quot;, 
       &quot;  '####': 25,\n&quot;, 
       &quot;  'his': 26,\n&quot;, 
       &quot;  'an': 27,\n&quot;, 
       &quot;  'this': 28,\n&quot;, 
       &quot;  'or': 29,\n&quot;, 
       &quot;  'their': 30,\n&quot;, 
       &quot;  'who': 31,\n&quot;, 
       &quot;  'they': 32,\n&quot;, 
       &quot;  'but': 33,\n&quot;, 
       &quot;  '$': 34,\n&quot;, 
       &quot;  'had': 35,\n&quot;, 
       &quot;  'year': 36,\n&quot;, 
       &quot;  'were': 37,\n&quot;, 
       &quot;  'we': 38,\n&quot;, 
       &quot;  'more': 39,\n&quot;, 
       &quot;  '###': 40,\n&quot;, 
       &quot;  'up': 41,\n&quot;, 
       &quot;  'been': 42,\n&quot;, 
       &quot;  'you': 43,\n&quot;, 
       &quot;  'its': 44,\n&quot;, 
       &quot;  'one': 45,\n&quot;, 
       &quot;  'about': 46,\n&quot;, 
       &quot;  'would': 47,\n&quot;, 
       &quot;  'which': 48,\n&quot;, 
       &quot;  'out': 49,\n&quot;, 
       &quot;  'can': 50,\n&quot;, 
       &quot;  'It': 51,\n&quot;, 
       &quot;  'all': 52,\n&quot;, 
       &quot;  'also': 53,\n&quot;, 
       &quot;  'two': 54,\n&quot;, 
       &quot;  'after': 55,\n&quot;, 
       &quot;  'first': 56,\n&quot;, 
       &quot;  'He': 57,\n&quot;, 
       &quot;  'do': 58,\n&quot;, 
       &quot;  'time': 59,\n&quot;, 
       &quot;  'than': 60,\n&quot;, 
       &quot;  'when': 61,\n&quot;, 
       &quot;  'We': 62,\n&quot;, 
       &quot;  'over': 63,\n&quot;, 
       &quot;  'last': 64,\n&quot;, 
       &quot;  'new': 65,\n&quot;, 
       &quot;  'other': 66,\n&quot;, 
       &quot;  'her': 67,\n&quot;, 
       &quot;  'people': 68,\n&quot;, 
       &quot;  'into': 69,\n&quot;, 
       &quot;  'In': 70,\n&quot;, 
       &quot;  'our': 71,\n&quot;, 
       &quot;  'there': 72,\n&quot;, 
       &quot;  'A': 73,\n&quot;, 
       &quot;  'she': 74,\n&quot;, 
       &quot;  'could': 75,\n&quot;, 
       &quot;  'just': 76,\n&quot;, 
       &quot;  'years': 77,\n&quot;, 
       &quot;  'some': 78,\n&quot;, 
       &quot;  'U.S.': 79,\n&quot;, 
       &quot;  'three': 80,\n&quot;, 
       &quot;  'million': 81,\n&quot;, 
       &quot;  'them': 82,\n&quot;, 
       &quot;  'what': 83,\n&quot;, 
       &quot;  'But': 84,\n&quot;, 
       &quot;  'so': 85,\n&quot;, 
       &quot;  'no': 86,\n&quot;, 
       &quot;  'like': 87,\n&quot;, 
       &quot;  'if': 88,\n&quot;, 
       &quot;  'only': 89,\n&quot;, 
       &quot;  'percent': 90,\n&quot;, 
       &quot;  'get': 91,\n&quot;, 
       &quot;  'did': 92,\n&quot;, 
       &quot;  'him': 93,\n&quot;, 
       &quot;  'game': 94,\n&quot;, 
       &quot;  'back': 95,\n&quot;, 
       &quot;  'because': 96,\n&quot;, 
       &quot;  'now': 97,\n&quot;, 
       &quot;  '#.#': 98,\n&quot;, 
       &quot;  'before': 99,\n&quot;, 
       &quot;  'company': 100,\n&quot;, 
       &quot;  'any': 101,\n&quot;, 
       &quot;  'team': 102,\n&quot;, 
       &quot;  'against': 103,\n&quot;, 
       &quot;  'off': 104,\n&quot;, 
       &quot;  'This': 105,\n&quot;, 
       &quot;  'most': 106,\n&quot;, 
       &quot;  'made': 107,\n&quot;, 
       &quot;  'through': 108,\n&quot;, 
       &quot;  'make': 109,\n&quot;, 
       &quot;  'second': 110,\n&quot;, 
       &quot;  'state': 111,\n&quot;, 
       &quot;  'well': 112,\n&quot;, 
       &quot;  'day': 113,\n&quot;, 
       &quot;  'season': 114,\n&quot;, 
       &quot;  'says': 115,\n&quot;, 
       &quot;  'week': 116,\n&quot;, 
       &quot;  'where': 117,\n&quot;, 
       &quot;  'while': 118,\n&quot;, 
       &quot;  'down': 119,\n&quot;, 
       &quot;  'being': 120,\n&quot;, 
       &quot;  'government': 121,\n&quot;, 
       &quot;  'your': 122,\n&quot;, 
       &quot;  '#-#': 123,\n&quot;, 
       &quot;  'home': 124,\n&quot;, 
       &quot;  'going': 125,\n&quot;, 
       &quot;  'my': 126,\n&quot;, 
       &quot;  'good': 127,\n&quot;, 
       &quot;  'They': 128,\n&quot;, 
       &quot;  \&quot;'re\&quot;: 129,\n&quot;, 
       &quot;  'should': 130,\n&quot;, 
       &quot;  'many': 131,\n&quot;, 
       &quot;  'way': 132,\n&quot;, 
       &quot;  'those': 133,\n&quot;, 
       &quot;  'four': 134,\n&quot;, 
       &quot;  'during': 135,\n&quot;, 
       &quot;  'such': 136,\n&quot;, 
       &quot;  'may': 137,\n&quot;, 
       &quot;  'very': 138,\n&quot;, 
       &quot;  'how': 139,\n&quot;, 
       &quot;  'since': 140,\n&quot;, 
       &quot;  'work': 141,\n&quot;, 
       &quot;  'take': 142,\n&quot;, 
       &quot;  'including': 143,\n&quot;, 
       &quot;  'high': 144,\n&quot;, 
       &quot;  'then': 145,\n&quot;, 
       &quot;  '%': 146,\n&quot;, 
       &quot;  'next': 147,\n&quot;, 
       &quot;  '#,###': 148,\n&quot;, 
       &quot;  'By': 149,\n&quot;, 
       &quot;  'much': 150,\n&quot;, 
       &quot;  'still': 151,\n&quot;, 
       &quot;  'go': 152,\n&quot;, 
       &quot;  'think': 153,\n&quot;, 
       &quot;  'old': 154,\n&quot;, 
       &quot;  'even': 155,\n&quot;, 
       &quot;  '#.##': 156,\n&quot;, 
       &quot;  'world': 157,\n&quot;, 
       &quot;  'see': 158,\n&quot;, 
       &quot;  'say': 159,\n&quot;, 
       &quot;  'business': 160,\n&quot;, 
       &quot;  'five': 161,\n&quot;, 
       &quot;  'told': 162,\n&quot;, 
       &quot;  'under': 163,\n&quot;, 
       &quot;  'us': 164,\n&quot;, 
       &quot;  '1': 165,\n&quot;, 
       &quot;  'these': 166,\n&quot;, 
       &quot;  'If': 167,\n&quot;, 
       &quot;  'right': 168,\n&quot;, 
       &quot;  'And': 169,\n&quot;, 
       &quot;  'me': 170,\n&quot;, 
       &quot;  'between': 171,\n&quot;, 
       &quot;  'play': 172,\n&quot;, 
       &quot;  'help': 173,\n&quot;, 
       &quot;  '##,###': 174,\n&quot;, 
       &quot;  'market': 175,\n&quot;, 
       &quot;  'That': 176,\n&quot;, 
       &quot;  'know': 177,\n&quot;, 
       &quot;  'end': 178,\n&quot;, 
       &quot;  'AP': 179,\n&quot;, 
       &quot;  'long': 180,\n&quot;, 
       &quot;  'information': 181,\n&quot;, 
       &quot;  'points': 182,\n&quot;, 
       &quot;  'does': 183,\n&quot;, 
       &quot;  'both': 184,\n&quot;, 
       &quot;  'There': 185,\n&quot;, 
       &quot;  'part': 186,\n&quot;, 
       &quot;  'around': 187,\n&quot;, 
       &quot;  'police': 188,\n&quot;, 
       &quot;  'want': 189,\n&quot;, 
       &quot;  \&quot;'ve\&quot;: 190,\n&quot;, 
       &quot;  'based': 191,\n&quot;, 
       &quot;  'For': 192,\n&quot;, 
       &quot;  'got': 193,\n&quot;, 
       &quot;  'third': 194,\n&quot;, 
       &quot;  'school': 195,\n&quot;, 
       &quot;  'left': 196,\n&quot;, 
       &quot;  'another': 197,\n&quot;, 
       &quot;  'country': 198,\n&quot;, 
       &quot;  'need': 199,\n&quot;, 
       &quot;  '2': 200,\n&quot;, 
       &quot;  'best': 201,\n&quot;, 
       &quot;  'win': 202,\n&quot;, 
       &quot;  'quarter': 203,\n&quot;, 
       &quot;  'use': 204,\n&quot;, 
       &quot;  'today': 205,\n&quot;, 
       &quot;  '##.#': 206,\n&quot;, 
       &quot;  'same': 207,\n&quot;, 
       &quot;  'public': 208,\n&quot;, 
       &quot;  'run': 209,\n&quot;, 
       &quot;  'Friday': 210,\n&quot;, 
       &quot;  'set': 211,\n&quot;, 
       &quot;  'month': 212,\n&quot;, 
       &quot;  'top': 213,\n&quot;, 
       &quot;  'billion': 214,\n&quot;, 
       &quot;  'Tuesday': 215,\n&quot;, 
       &quot;  'come': 216,\n&quot;, 
       &quot;  'Monday': 217,\n&quot;, 
       &quot;  'She': 218,\n&quot;, 
       &quot;  'city': 219,\n&quot;, 
       &quot;  'place': 220,\n&quot;, 
       &quot;  'night': 221,\n&quot;, 
       &quot;  'six': 222,\n&quot;, 
       &quot;  'each': 223,\n&quot;, 
       &quot;  'Thursday': 224,\n&quot;, 
       &quot;  '###,###': 225,\n&quot;, 
       &quot;  'Wednesday': 226,\n&quot;, 
       &quot;  'here': 227,\n&quot;, 
       &quot;  'You': 228,\n&quot;, 
       &quot;  'group': 229,\n&quot;, 
       &quot;  'really': 230,\n&quot;, 
       &quot;  'found': 231,\n&quot;, 
       &quot;  'As': 232,\n&quot;, 
       &quot;  'used': 233,\n&quot;, 
       &quot;  '3': 234,\n&quot;, 
       &quot;  'lot': 235,\n&quot;, 
       &quot;  \&quot;'m\&quot;: 236,\n&quot;, 
       &quot;  'money': 237,\n&quot;, 
       &quot;  'put': 238,\n&quot;, 
       &quot;  'games': 239,\n&quot;, 
       &quot;  'support': 240,\n&quot;, 
       &quot;  'program': 241,\n&quot;, 
       &quot;  'half': 242,\n&quot;, 
       &quot;  'report': 243,\n&quot;, 
       &quot;  'family': 244,\n&quot;, 
       &quot;  'months': 245,\n&quot;, 
       &quot;  'number': 246,\n&quot;, 
       &quot;  'officials': 247,\n&quot;, 
       &quot;  'am': 248,\n&quot;, 
       &quot;  'former': 249,\n&quot;, 
       &quot;  'own': 250,\n&quot;, 
       &quot;  'man': 251,\n&quot;, 
       &quot;  'Saturday': 252,\n&quot;, 
       &quot;  'too': 253,\n&quot;, 
       &quot;  'better': 254,\n&quot;, 
       &quot;  'days': 255,\n&quot;, 
       &quot;  'came': 256,\n&quot;, 
       &quot;  'lead': 257,\n&quot;, 
       &quot;  'life': 258,\n&quot;, 
       &quot;  'American': 259,\n&quot;, 
       &quot;  '##-##': 260,\n&quot;, 
       &quot;  'show': 261,\n&quot;, 
       &quot;  'past': 262,\n&quot;, 
       &quot;  'took': 263,\n&quot;, 
       &quot;  'added': 264,\n&quot;, 
       &quot;  'expected': 265,\n&quot;, 
       &quot;  'called': 266,\n&quot;, 
       &quot;  'great': 267,\n&quot;, 
       &quot;  'State': 268,\n&quot;, 
       &quot;  'services': 269,\n&quot;, 
       &quot;  'children': 270,\n&quot;, 
       &quot;  'hit': 271,\n&quot;, 
       &quot;  'area': 272,\n&quot;, 
       &quot;  'system': 273,\n&quot;, 
       &quot;  'every': 274,\n&quot;, 
       &quot;  'pm': 275,\n&quot;, 
       &quot;  'big': 276,\n&quot;, 
       &quot;  'service': 277,\n&quot;, 
       &quot;  'few': 278,\n&quot;, 
       &quot;  'per': 279,\n&quot;, 
       &quot;  'members': 280,\n&quot;, 
       &quot;  'Sunday': 281,\n&quot;, 
       &quot;  'early': 282,\n&quot;, 
       &quot;  'point': 283,\n&quot;, 
       &quot;  'start': 284,\n&quot;, 
       &quot;  'companies': 285,\n&quot;, 
       &quot;  'little': 286,\n&quot;, 
       &quot;  '&amp;': 287,\n&quot;, 
       &quot;  'case': 288,\n&quot;, 
       &quot;  'ago': 289,\n&quot;, 
       &quot;  'local': 290,\n&quot;, 
       &quot;  'according': 291,\n&quot;, 
       &quot;  'never': 292,\n&quot;, 
       &quot;  '5': 293,\n&quot;, 
       &quot;  'without': 294,\n&quot;, 
       &quot;  'sales': 295,\n&quot;, 
       &quot;  'until': 296,\n&quot;, 
       &quot;  'went': 297,\n&quot;, 
       &quot;  'players': 298,\n&quot;, 
       &quot;  '##th': 299,\n&quot;, 
       &quot;  'New_York': 300,\n&quot;, 
       &quot;  'won': 301,\n&quot;, 
       &quot;  'financial': 302,\n&quot;, 
       &quot;  'news': 303,\n&quot;, 
       &quot;  '4': 304,\n&quot;, 
       &quot;  'When': 305,\n&quot;, 
       &quot;  'share': 306,\n&quot;, 
       &quot;  'several': 307,\n&quot;, 
       &quot;  'free': 308,\n&quot;, 
       &quot;  'away': 309,\n&quot;, 
       &quot;  '##.##': 310,\n&quot;, 
       &quot;  'already': 311,\n&quot;, 
       &quot;  'On': 312,\n&quot;, 
       &quot;  'industry': 313,\n&quot;, 
       &quot;  \&quot;'ll\&quot;: 314,\n&quot;, 
       &quot;  'call': 315,\n&quot;, 
       &quot;  'With': 316,\n&quot;, 
       &quot;  'students': 317,\n&quot;, 
       &quot;  'line': 318,\n&quot;, 
       &quot;  'available': 319,\n&quot;, 
       &quot;  'County': 320,\n&quot;, 
       &quot;  'making': 321,\n&quot;, 
       &quot;  'held': 322,\n&quot;, 
       &quot;  'final': 323,\n&quot;, 
       &quot;  '#:##': 324,\n&quot;, 
       &quot;  'power': 325,\n&quot;, 
       &quot;  'plan': 326,\n&quot;, 
       &quot;  'might': 327,\n&quot;, 
       &quot;  'least': 328,\n&quot;, 
       &quot;  'look': 329,\n&quot;, 
       &quot;  'forward': 330,\n&quot;, 
       &quot;  'give': 331,\n&quot;, 
       &quot;  'At': 332,\n&quot;, 
       &quot;  'again': 333,\n&quot;, 
       &quot;  'later': 334,\n&quot;, 
       &quot;  'full': 335,\n&quot;, 
       &quot;  'must': 336,\n&quot;, 
       &quot;  'things': 337,\n&quot;, 
       &quot;  'major': 338,\n&quot;, 
       &quot;  'community': 339,\n&quot;, 
       &quot;  'announced': 340,\n&quot;, 
       &quot;  'open': 341,\n&quot;, 
       &quot;  'record': 342,\n&quot;, 
       &quot;  'reported': 343,\n&quot;, 
       &quot;  'court': 344,\n&quot;, 
       &quot;  'working': 345,\n&quot;, 
       &quot;  'able': 346,\n&quot;, 
       &quot;  'something': 347,\n&quot;, 
       &quot;  'president': 348,\n&quot;, 
       &quot;  'meeting': 349,\n&quot;, 
       &quot;  'keep': 350,\n&quot;, 
       &quot;  'March': 351,\n&quot;, 
       &quot;  'future': 352,\n&quot;, 
       &quot;  'far': 353,\n&quot;, 
       &quot;  'deal': 354,\n&quot;, 
       &quot;  'City': 355,\n&quot;, 
       &quot;  'May': 356,\n&quot;, 
       &quot;  'development': 357,\n&quot;, 
       &quot;  'University': 358,\n&quot;, 
       &quot;  'find': 359,\n&quot;, 
       &quot;  'times': 360,\n&quot;, 
       &quot;  'After': 361,\n&quot;, 
       &quot;  'office': 362,\n&quot;, 
       &quot;  'led': 363,\n&quot;, 
       &quot;  'among': 364,\n&quot;, 
       &quot;  'June': 365,\n&quot;, 
       &quot;  'increase': 366,\n&quot;, 
       &quot;  'China': 367,\n&quot;, 
       &quot;  'John': 368,\n&quot;, 
       &quot;  'whether': 369,\n&quot;, 
       &quot;  'cost': 370,\n&quot;, 
       &quot;  'security': 371,\n&quot;, 
       &quot;  'job': 372,\n&quot;, 
       &quot;  'less': 373,\n&quot;, 
       &quot;  'head': 374,\n&quot;, 
       &quot;  'seven': 375,\n&quot;, 
       &quot;  'growth': 376,\n&quot;, 
       &quot;  'lost': 377,\n&quot;, 
       &quot;  'pay': 378,\n&quot;, 
       &quot;  'looking': 379,\n&quot;, 
       &quot;  'provide': 380,\n&quot;, 
       &quot;  '6': 381,\n&quot;, 
       &quot;  'To': 382,\n&quot;, 
       &quot;  'plans': 383,\n&quot;, 
       &quot;  'products': 384,\n&quot;, 
       &quot;  'car': 385,\n&quot;, 
       &quot;  'recent': 386,\n&quot;, 
       &quot;  'hard': 387,\n&quot;, 
       &quot;  'always': 388,\n&quot;, 
       &quot;  'include': 389,\n&quot;, 
       &quot;  'women': 390,\n&quot;, 
       &quot;  'across': 391,\n&quot;, 
       &quot;  'tax': 392,\n&quot;, 
       &quot;  'water': 393,\n&quot;, 
       &quot;  'April': 394,\n&quot;, 
       &quot;  'continue': 395,\n&quot;, 
       &quot;  'important': 396,\n&quot;, 
       &quot;  'different': 397,\n&quot;, 
       &quot;  'close': 398,\n&quot;, 
       &quot;  '7': 399,\n&quot;, 
       &quot;  'One': 400,\n&quot;, 
       &quot;  'late': 401,\n&quot;, 
       &quot;  'decision': 402,\n&quot;, 
       &quot;  'current': 403,\n&quot;, 
       &quot;  'law': 404,\n&quot;, 
       &quot;  'within': 405,\n&quot;, 
       &quot;  'along': 406,\n&quot;, 
       &quot;  'played': 407,\n&quot;, 
       &quot;  'move': 408,\n&quot;, 
       &quot;  'United_States': 409,\n&quot;, 
       &quot;  'enough': 410,\n&quot;, 
       &quot;  'become': 411,\n&quot;, 
       &quot;  'side': 412,\n&quot;, 
       &quot;  'national': 413,\n&quot;, 
       &quot;  'Inc.': 414,\n&quot;, 
       &quot;  'results': 415,\n&quot;, 
       &quot;  'level': 416,\n&quot;, 
       &quot;  'loss': 417,\n&quot;, 
       &quot;  'economic': 418,\n&quot;, 
       &quot;  'coach': 419,\n&quot;, 
       &quot;  'near': 420,\n&quot;, 
       &quot;  'getting': 421,\n&quot;, 
       &quot;  'price': 422,\n&quot;, 
       &quot;  'Department': 423,\n&quot;, 
       &quot;  'event': 424,\n&quot;, 
       &quot;  'fourth': 425,\n&quot;, 
       &quot;  'change': 426,\n&quot;, 
       &quot;  'All': 427,\n&quot;, 
       &quot;  'small': 428,\n&quot;, 
       &quot;  'board': 429,\n&quot;, 
       &quot;  'National': 430,\n&quot;, 
       &quot;  'So': 431,\n&quot;, 
       &quot;  'goal': 432,\n&quot;, 
       &quot;  'taken': 433,\n&quot;, 
       &quot;  'field': 434,\n&quot;, 
       &quot;  'prices': 435,\n&quot;, 
       &quot;  'weeks': 436,\n&quot;, 
       &quot;  'men': 437,\n&quot;, 
       &quot;  'asked': 438,\n&quot;, 
       &quot;  'eight': 439,\n&quot;, 
       &quot;  'data': 440,\n&quot;, 
       &quot;  'shot': 441,\n&quot;, 
       &quot;  'New': 442,\n&quot;, 
       &quot;  'started': 443,\n&quot;, 
       &quot;  'July': 444,\n&quot;, 
       &quot;  'director': 445,\n&quot;, 
       &quot;  'President': 446,\n&quot;, 
       &quot;  'party': 447,\n&quot;, 
       &quot;  'federal': 448,\n&quot;, 
       &quot;  'done': 449,\n&quot;, 
       &quot;  'political': 450,\n&quot;, 
       &quot;  'minutes': 451,\n&quot;, 
       &quot;  'taking': 452,\n&quot;, 
       &quot;  'Company': 453,\n&quot;, 
       &quot;  'technology': 454,\n&quot;, 
       &quot;  'project': 455,\n&quot;, 
       &quot;  'center': 456,\n&quot;, 
       &quot;  'leading': 457,\n&quot;, 
       &quot;  'issue': 458,\n&quot;, 
       &quot;  'though': 459,\n&quot;, 
       &quot;  'having': 460,\n&quot;, 
       &quot;  'period': 461,\n&quot;, 
       &quot;  'likely': 462,\n&quot;, 
       &quot;  'scored': 463,\n&quot;, 
       &quot;  '8': 464,\n&quot;, 
       &quot;  'strong': 465,\n&quot;, 
       &quot;  'series': 466,\n&quot;, 
       &quot;  'military': 467,\n&quot;, 
       &quot;  'seen': 468,\n&quot;, 
       &quot;  'trying': 469,\n&quot;, 
       &quot;  'What': 470,\n&quot;, 
       &quot;  'coming': 471,\n&quot;, 
       &quot;  'process': 472,\n&quot;, 
       &quot;  'building': 473,\n&quot;, 
       &quot;  'behind': 474,\n&quot;, 
       &quot;  'performance': 475,\n&quot;, 
       &quot;  'management': 476,\n&quot;, 
       &quot;  'Iraq': 477,\n&quot;, 
       &quot;  'saying': 478,\n&quot;, 
       &quot;  'earlier': 479,\n&quot;, 
       &quot;  'believe': 480,\n&quot;, 
       &quot;  'oil': 481,\n&quot;, 
       &quot;  'given': 482,\n&quot;, 
       &quot;  'Police': 483,\n&quot;, 
       &quot;  'customers': 484,\n&quot;, 
       &quot;  'due': 485,\n&quot;, 
       &quot;  'following': 486,\n&quot;, 
       &quot;  'term': 487,\n&quot;, 
       &quot;  'others': 488,\n&quot;, 
       &quot;  'statement': 489,\n&quot;, 
       &quot;  'international': 490,\n&quot;, 
       &quot;  'economy': 491,\n&quot;, 
       &quot;  'health': 492,\n&quot;, 
       &quot;  'thing': 493,\n&quot;, 
       &quot;  'Obama': 494,\n&quot;, 
       &quot;  'return': 495,\n&quot;, 
       &quot;  'killed': 496,\n&quot;, 
       &quot;  'Washington': 497,\n&quot;, 
       &quot;  'further': 498,\n&quot;, 
       &quot;  'However': 499,\n&quot;, 
       &quot;  'doing': 500,\n&quot;, 
       &quot;  'face': 501,\n&quot;, 
       &quot;  'low': 502,\n&quot;, 
       &quot;  'higher': 503,\n&quot;, 
       &quot;  'site': 504,\n&quot;, 
       &quot;  'once': 505,\n&quot;, 
       &quot;  'yet': 506,\n&quot;, 
       &quot;  'hours': 507,\n&quot;, 
       &quot;  'America': 508,\n&quot;, 
       &quot;  'control': 509,\n&quot;, 
       &quot;  'received': 510,\n&quot;, 
       &quot;  'rate': 511,\n&quot;, 
       &quot;  'career': 512,\n&quot;, 
       &quot;  'Bush': 513,\n&quot;, 
       &quot;  'teams': 514,\n&quot;, 
       &quot;  'known': 515,\n&quot;, 
       &quot;  'offer': 516,\n&quot;, 
       &quot;  'race': 517,\n&quot;, 
       &quot;  'ever': 518,\n&quot;, 
       &quot;  'experience': 519,\n&quot;, 
       &quot;  'playing': 520,\n&quot;, 
       &quot;  'name': 521,\n&quot;, 
       &quot;  'possible': 522,\n&quot;, 
       &quot;  'countries': 523,\n&quot;, 
       &quot;  'Mr.': 524,\n&quot;, 
       &quot;  'average': 525,\n&quot;, 
       &quot;  'together': 526,\n&quot;, 
       &quot;  'using': 527,\n&quot;, 
       &quot;  '9': 528,\n&quot;, 
       &quot;  'cut': 529,\n&quot;, 
       &quot;  'While': 530,\n&quot;, 
       &quot;  'total': 531,\n&quot;, 
       &quot;  'round': 532,\n&quot;, 
       &quot;  'young': 533,\n&quot;, 
       &quot;  'nearly': 534,\n&quot;, 
       &quot;  'shares': 535,\n&quot;, 
       &quot;  'member': 536,\n&quot;, 
       &quot;  'campaign': 537,\n&quot;, 
       &quot;  'media': 538,\n&quot;, 
       &quot;  'needs': 539,\n&quot;, 
       &quot;  'why': 540,\n&quot;, 
       &quot;  'house': 541,\n&quot;, 
       &quot;  'issues': 542,\n&quot;, 
       &quot;  'costs': 543,\n&quot;, 
       &quot;  'fire': 544,\n&quot;, 
       &quot;  '##-#': 545,\n&quot;, 
       &quot;  'victory': 546,\n&quot;, 
       &quot;  'player': 547,\n&quot;, 
       &quot;  'began': 548,\n&quot;, 
       &quot;  'sure': 549,\n&quot;, 
       &quot;  'story': 550,\n&quot;, 
       &quot;  'per_cent': 551,\n&quot;, 
       &quot;  'North': 552,\n&quot;, 
       &quot;  'His': 553,\n&quot;, 
       &quot;  'staff': 554,\n&quot;, 
       &quot;  'order': 555,\n&quot;, 
       &quot;  'war': 556,\n&quot;, 
       &quot;  'large': 557,\n&quot;, 
       &quot;  'interest': 558,\n&quot;, 
       &quot;  'stock': 559,\n&quot;, 
       &quot;  'food': 560,\n&quot;, 
       &quot;  'research': 561,\n&quot;, 
       &quot;  'key': 562,\n&quot;, 
       &quot;  'India': 563,\n&quot;, 
       &quot;  'South': 564,\n&quot;, 
       &quot;  'morning': 565,\n&quot;, 
       &quot;  'conference': 566,\n&quot;, 
       &quot;  'senior': 567,\n&quot;, 
       &quot;  'global': 568,\n&quot;, 
       &quot;  'Center': 569,\n&quot;, 
       &quot;  'death': 570,\n&quot;, 
       &quot;  'person': 571,\n&quot;, 
       &quot;  'thought': 572,\n&quot;, 
       &quot;  'gave': 573,\n&quot;, 
       &quot;  'feel': 574,\n&quot;, 
       &quot;  'energy': 575,\n&quot;, 
       &quot;  'history': 576,\n&quot;, 
       &quot;  'recently': 577,\n&quot;, 
       &quot;  'largest': 578,\n&quot;, 
       &quot;  'No.': 579,\n&quot;, 
       &quot;  'general': 580,\n&quot;, 
       &quot;  'official': 581,\n&quot;, 
       &quot;  'released': 582,\n&quot;, 
       &quot;  'wanted': 583,\n&quot;, 
       &quot;  'meet': 584,\n&quot;, 
       &quot;  'short': 585,\n&quot;, 
       &quot;  'outside': 586,\n&quot;, 
       &quot;  'running': 587,\n&quot;, 
       &quot;  'live': 588,\n&quot;, 
       &quot;  'ball': 589,\n&quot;, 
       &quot;  'online': 590,\n&quot;, 
       &quot;  'real': 591,\n&quot;, 
       &quot;  'position': 592,\n&quot;, 
       &quot;  'fact': 593,\n&quot;, 
       &quot;  'fell': 594,\n&quot;, 
       &quot;  'nine': 595,\n&quot;, 
       &quot;  'December': 596,\n&quot;, 
       &quot;  'front': 597,\n&quot;, 
       &quot;  'action': 598,\n&quot;, 
       &quot;  'defense': 599,\n&quot;, 
       &quot;  'problem': 600,\n&quot;, 
       &quot;  'problems': 601,\n&quot;, 
       &quot;  'Mr': 602,\n&quot;, 
       &quot;  'nation': 603,\n&quot;, 
       &quot;  'needed': 604,\n&quot;, 
       &quot;  'special': 605,\n&quot;, 
       &quot;  'January': 606,\n&quot;, 
       &quot;  'almost': 607,\n&quot;, 
       &quot;  'chance': 608,\n&quot;, 
       &quot;  \&quot;'d\&quot;: 609,\n&quot;, 
       &quot;  'result': 610,\n&quot;, 
       &quot;  'West': 611,\n&quot;, 
       &quot;  'September': 612,\n&quot;, 
       &quot;  'reports': 613,\n&quot;, 
       &quot;  'leader': 614,\n&quot;, 
       &quot;  'investment': 615,\n&quot;, 
       &quot;  'yesterday': 616,\n&quot;, 
       &quot;  'Some': 617,\n&quot;, 
       &quot;  'leaders': 618,\n&quot;, 
       &quot;  'ahead': 619,\n&quot;, 
       &quot;  'production': 620,\n&quot;, 
       &quot;  'comes': 621,\n&quot;, 
       &quot;  'No': 622,\n&quot;, 
       &quot;  'runs': 623,\n&quot;, 
       &quot;  'match': 624,\n&quot;, 
       &quot;  'role': 625,\n&quot;, 
       &quot;  'kind': 626,\n&quot;, 
       &quot;  'try': 627,\n&quot;, 
       &quot;  'ended': 628,\n&quot;, 
       &quot;  'risk': 629,\n&quot;, 
       &quot;  'areas': 630,\n&quot;, 
       &quot;  'election': 631,\n&quot;, 
       &quot;  'workers': 632,\n&quot;, 
       &quot;  'visit': 633,\n&quot;, 
       &quot;  'bring': 634,\n&quot;, 
       &quot;  'road': 635,\n&quot;, 
       &quot;  'music': 636,\n&quot;, 
       &quot;  'study': 637,\n&quot;, 
       &quot;  'makes': 638,\n&quot;, 
       &quot;  'often': 639,\n&quot;, 
       &quot;  'release': 640,\n&quot;, 
       &quot;  'woman': 641,\n&quot;, 
       &quot;  'vote': 642,\n&quot;, 
       &quot;  'care': 643,\n&quot;, 
       &quot;  'town': 644,\n&quot;, 
       &quot;  'clear': 645,\n&quot;, 
       &quot;  'comment': 646,\n&quot;, 
       &quot;  'budget': 647,\n&quot;, 
       &quot;  'potential': 648,\n&quot;, 
       &quot;  'single': 649,\n&quot;, 
       &quot;  'markets': 650,\n&quot;, 
       &quot;  'policy': 651,\n&quot;, 
       &quot;  'capital': 652,\n&quot;, 
       &quot;  'saw': 653,\n&quot;, 
       &quot;  'access': 654,\n&quot;, 
       &quot;  'weekend': 655,\n&quot;, 
       &quot;  'operations': 656,\n&quot;, 
       &quot;  'whose': 657,\n&quot;, 
       &quot;  'net': 658,\n&quot;, 
       &quot;  'House': 659,\n&quot;, 
       &quot;  'hand': 660,\n&quot;, 
       &quot;  'increased': 661,\n&quot;, 
       &quot;  'charges': 662,\n&quot;, 
       &quot;  'winning': 663,\n&quot;, 
       &quot;  'trade': 664,\n&quot;, 
       &quot;  'These': 665,\n&quot;, 
       &quot;  'income': 666,\n&quot;, 
       &quot;  'value': 667,\n&quot;, 
       &quot;  'involved': 668,\n&quot;, 
       &quot;  'Bank': 669,\n&quot;, 
       &quot;  'November': 670,\n&quot;, 
       &quot;  'bill': 671,\n&quot;, 
       &quot;  'compared': 672,\n&quot;, 
       &quot;  'anything': 673,\n&quot;, 
       &quot;  'manager': 674,\n&quot;, 
       &quot;  'Texas': 675,\n&quot;, 
       &quot;  'property': 676,\n&quot;, 
       &quot;  'stop': 677,\n&quot;, 
       &quot;  'annual': 678,\n&quot;, 
       &quot;  'private': 679,\n&quot;, 
       &quot;  'contract': 680,\n&quot;, 
       &quot;  'died': 681,\n&quot;, 
       &quot;  'Now': 682,\n&quot;, 
       &quot;  'hope': 683,\n&quot;, 
       &quot;  'product': 684,\n&quot;, 
       &quot;  'fans': 685,\n&quot;, 
       &quot;  'lower': 686,\n&quot;, 
       &quot;  'demand': 687,\n&quot;, 
       &quot;  'News': 688,\n&quot;, 
       &quot;  'David': 689,\n&quot;, 
       &quot;  'club': 690,\n&quot;, 
       &quot;  'comments': 691,\n&quot;, 
       &quot;  'film': 692,\n&quot;, 
       &quot;  'yards': 693,\n&quot;, 
       &quot;  'quality': 694,\n&quot;, 
       &quot;  'currently': 695,\n&quot;, 
       &quot;  'events': 696,\n&quot;, 
       &quot;  'addition': 697,\n&quot;, 
       &quot;  'couple': 698,\n&quot;, 
       &quot;  'schools': 699,\n&quot;, 
       &quot;  'attack': 700,\n&quot;, 
       &quot;  'region': 701,\n&quot;, 
       &quot;  'latest': 702,\n&quot;, 
       &quot;  'opportunity': 703,\n&quot;, 
       &quot;  'worked': 704,\n&quot;, 
       &quot;  'course': 705,\n&quot;, 
       &quot;  'bad': 706,\n&quot;, 
       &quot;  'fall': 707,\n&quot;, 
       &quot;  'Group': 708,\n&quot;, 
       &quot;  'October': 709,\n&quot;, 
       &quot;  'jobs': 710,\n&quot;, 
       &quot;  'list': 711,\n&quot;, 
       &quot;  'let': 712,\n&quot;, 
       &quot;  'however': 713,\n&quot;, 
       &quot;  'chief': 714,\n&quot;, 
       &quot;  'summer': 715,\n&quot;, 
       &quot;  'programs': 716,\n&quot;, 
       &quot;  'According': 717,\n&quot;, 
       &quot;  'revenue': 718,\n&quot;, 
       &quot;  'Our': 719,\n&quot;, 
       &quot;  'rose': 720,\n&quot;, 
       &quot;  'previous': 721,\n&quot;, 
       &quot;  'TV': 722,\n&quot;, 
       &quot;  'football': 723,\n&quot;, 
       &quot;  'biggest': 724,\n&quot;, 
       &quot;  'employees': 725,\n&quot;, 
       &quot;  'changes': 726,\n&quot;, 
       &quot;  'residents': 727,\n&quot;, 
       &quot;  'means': 728,\n&quot;, 
       &quot;  'agreement': 729,\n&quot;, 
       &quot;  'includes': 730,\n&quot;, 
       &quot;  'post': 731,\n&quot;, 
       &quot;  'Canada': 732,\n&quot;, 
       &quot;  'probably': 733,\n&quot;, 
       &quot;  'related': 734,\n&quot;, 
       &quot;  'training': 735,\n&quot;, 
       &quot;  'allowed': 736,\n&quot;, 
       &quot;  'class': 737,\n&quot;, 
       &quot;  'bit': 738,\n&quot;, 
       &quot;  'video': 739,\n&quot;, 
       &quot;  'Michael': 740,\n&quot;, 
       &quot;  'An': 741,\n&quot;, 
       &quot;  'sent': 742,\n&quot;, 
       &quot;  'education': 743,\n&quot;, 
       &quot;  'states': 744,\n&quot;, 
       &quot;  'straight': 745,\n&quot;, 
       &quot;  'love': 746,\n&quot;, 
       &quot;  'beat': 747,\n&quot;, 
       &quot;  'hold': 748,\n&quot;, 
       &quot;  'turn': 749,\n&quot;, 
       &quot;  'finished': 750,\n&quot;, 
       &quot;  'network': 751,\n&quot;, 
       &quot;  'Smith': 752,\n&quot;, 
       &quot;  'buy': 753,\n&quot;, 
       &quot;  'foreign': 754,\n&quot;, 
       &quot;  'especially': 755,\n&quot;, 
       &quot;  'groups': 756,\n&quot;, 
       &quot;  'wants': 757,\n&quot;, 
       &quot;  'title': 758,\n&quot;, 
       &quot;  'included': 759,\n&quot;, 
       &quot;  'turned': 760,\n&quot;, 
       &quot;  'bank': 761,\n&quot;, 
       &quot;  'Florida': 762,\n&quot;, 
       &quot;  'efforts': 763,\n&quot;, 
       &quot;  'personal': 764,\n&quot;, 
       &quot;  'businesses': 765,\n&quot;, 
       &quot;  'August': 766,\n&quot;, 
       &quot;  'California': 767,\n&quot;, 
       &quot;  'situation': 768,\n&quot;, 
       &quot;  'district': 769,\n&quot;, 
       &quot;  'allow': 770,\n&quot;, 
       &quot;  'helped': 771,\n&quot;, 
       &quot;  'body': 772,\n&quot;, 
       &quot;  'nothing': 773,\n&quot;, 
       &quot;  'soon': 774,\n&quot;, 
       &quot;  'safety': 775,\n&quot;, 
       &quot;  'officer': 776,\n&quot;, 
       &quot;  'cents': 777,\n&quot;, 
       &quot;  'Europe': 778,\n&quot;, 
       &quot;  'St.': 779,\n&quot;, 
       &quot;  'additional': 780,\n&quot;, 
       &quot;  'spokesman': 781,\n&quot;, 
       &quot;  'February': 782,\n&quot;, 
       &quot;  'wife': 783,\n&quot;, 
       &quot;  'showed': 784,\n&quot;, 
       &quot;  'leave': 785,\n&quot;, 
       &quot;  'investors': 786,\n&quot;, 
       &quot;  'parents': 787,\n&quot;, 
       &quot;  'medical': 788,\n&quot;, 
       &quot;  'spending': 789,\n&quot;, 
       &quot;  'non': 790,\n&quot;, 
       &quot;  'London': 791,\n&quot;, 
       &quot;  'Council': 792,\n&quot;, 
       &quot;  'matter': 793,\n&quot;, 
       &quot;  'spent': 794,\n&quot;, 
       &quot;  'child': 795,\n&quot;, 
       &quot;  'World': 796,\n&quot;, 
       &quot;  'effort': 797,\n&quot;, 
       &quot;  'opening': 798,\n&quot;, 
       &quot;  'either': 799,\n&quot;, 
       &quot;  'range': 800,\n&quot;, 
       &quot;  'question': 801,\n&quot;, 
       &quot;  'European': 802,\n&quot;, 
       &quot;  'goals': 803,\n&quot;, 
       &quot;  'administration': 804,\n&quot;, 
       &quot;  'friends': 805,\n&quot;, 
       &quot;  'himself': 806,\n&quot;, 
       &quot;  'shows': 807,\n&quot;, 
       &quot;  'difficult': 808,\n&quot;, 
       &quot;  'kids': 809,\n&quot;, 
       &quot;  'paid': 810,\n&quot;, 
       &quot;  'create': 811,\n&quot;, 
       &quot;  'cash': 812,\n&quot;, 
       &quot;  'age': 813,\n&quot;, 
       &quot;  'league': 814,\n&quot;, 
       &quot;  'form': 815,\n&quot;, 
       &quot;  'impact': 816,\n&quot;, 
       &quot;  'drive': 817,\n&quot;, 
       &quot;  'someone': 818,\n&quot;, 
       &quot;  'became': 819,\n&quot;, 
       &quot;  'stay': 820,\n&quot;, 
       &quot;  'fight': 821,\n&quot;, 
       &quot;  'significant': 822,\n&quot;, 
       &quot;  'firm': 823,\n&quot;, 
       &quot;  'Senate': 824,\n&quot;, 
       &quot;  'hospital': 825,\n&quot;, 
       &quot;  'charged': 826,\n&quot;, 
       &quot;  'operating': 827,\n&quot;, 
       &quot;  'main': 828,\n&quot;, 
       &quot;  'book': 829,\n&quot;, 
       &quot;  'success': 830,\n&quot;, 
       &quot;  'son': 831,\n&quot;, 
       &quot;  'trading': 832,\n&quot;, 
       &quot;  '###-####': 833,\n&quot;, 
       &quot;  'focus': 834,\n&quot;, 
       &quot;  'room': 835,\n&quot;, 
       &quot;  'continued': 836,\n&quot;, 
       &quot;  'Congress': 837,\n&quot;, 
       &quot;  'everything': 838,\n&quot;, 
       &quot;  'Park': 839,\n&quot;, 
       &quot;  'agency': 840,\n&quot;, 
       &quot;  'brought': 841,\n&quot;, 
       &quot;  'talk': 842,\n&quot;, 
       &quot;  'break': 843,\n&quot;, 
       &quot;  'air': 844,\n&quot;, 
       &quot;  'software': 845,\n&quot;, 
       &quot;  'decided': 846,\n&quot;, 
       &quot;  'Do': 847,\n&quot;, 
       &quot;  'ready': 848,\n&quot;, 
       &quot;  'arrested': 849,\n&quot;, 
       &quot;  'track': 850,\n&quot;, 
       &quot;  'provides': 851,\n&quot;, 
       &quot;  'mother': 852,\n&quot;, 
       &quot;  'base': 853,\n&quot;, 
       &quot;  'trial': 854,\n&quot;, 
       &quot;  'phone': 855,\n&quot;, 
       &quot;  'My': 856,\n&quot;, 
       &quot;  'build': 857,\n&quot;, 
       &quot;  'conditions': 858,\n&quot;, 
       &quot;  'rest': 859,\n&quot;, 
       &quot;  'Johnson': 860,\n&quot;, 
       &quot;  'terms': 861,\n&quot;, 
       &quot;  'expect': 862,\n&quot;, 
       &quot;  'England': 863,\n&quot;, 
       &quot;  'Israel': 864,\n&quot;, 
       &quot;  'despite': 865,\n&quot;, 
       &quot;  'closed': 866,\n&quot;, 
       &quot;  'starting': 867,\n&quot;, 
       &quot;  'provided': 868,\n&quot;, 
       &quot;  'pressure': 869,\n&quot;, 
       &quot;  'lives': 870,\n&quot;, 
       &quot;  'step': 871,\n&quot;, 
       &quot;  'remain': 872,\n&quot;, 
       &quot;  'similar': 873,\n&quot;, 
       &quot;  'charge': 874,\n&quot;, 
       &quot;  'date': 875,\n&quot;, 
       &quot;  'whole': 876,\n&quot;, 
       &quot;  'land': 877,\n&quot;, 
       &quot;  'growing': 878,\n&quot;, 
       &quot;  'James': 879,\n&quot;, 
       &quot;  'Internet': 880,\n&quot;, 
       &quot;  'projects': 881,\n&quot;, 
       &quot;  'British': 882,\n&quot;, 
       &quot;  'cases': 883,\n&quot;, 
       &quot;  'ground': 884,\n&quot;, 
       &quot;  'legal': 885,\n&quot;, 
       &quot;  'International': 886,\n&quot;, 
       &quot;  'agreed': 887,\n&quot;, 
       &quot;  'tell': 888,\n&quot;, 
       &quot;  'test': 889,\n&quot;, 
       &quot;  'everyone': 890,\n&quot;, 
       &quot;  'pretty': 891,\n&quot;, 
       &quot;  'authorities': 892,\n&quot;, 
       &quot;  'Two': 893,\n&quot;, 
       &quot;  'above': 894,\n&quot;, 
       &quot;  'moved': 895,\n&quot;, 
       &quot;  'profit': 896,\n&quot;, 
       &quot;  'throughout': 897,\n&quot;, 
       &quot;  'inside': 898,\n&quot;, 
       &quot;  'ability': 899,\n&quot;, 
       &quot;  'overall': 900,\n&quot;, 
       &quot;  'pass': 901,\n&quot;, 
       &quot;  'officers': 902,\n&quot;, 
       &quot;  'rather': 903,\n&quot;, 
       &quot;  'Australia': 904,\n&quot;, 
       &quot;  'actually': 905,\n&quot;, 
       &quot;  'county': 906,\n&quot;, 
       &quot;  'amount': 907,\n&quot;, 
       &quot;  'scheduled': 908,\n&quot;, 
       &quot;  'themselves': 909,\n&quot;, 
       &quot;  'organization': 910,\n&quot;, 
       &quot;  'giving': 911,\n&quot;, 
       &quot;  'credit': 912,\n&quot;, 
       &quot;  'father': 913,\n&quot;, 
       &quot;  'drug': 914,\n&quot;, 
       &quot;  'investigation': 915,\n&quot;, 
       &quot;  'families': 916,\n&quot;, 
       &quot;  'Republican': 917,\n&quot;, 
       &quot;  'funds': 918,\n&quot;, 
       &quot;  'patients': 919,\n&quot;, 
       &quot;  'takes': 920,\n&quot;, 
       &quot;  'systems': 921,\n&quot;, 
       &quot;  'Japan': 922,\n&quot;, 
       &quot;  'complete': 923,\n&quot;, 
       &quot;  'sold': 924,\n&quot;, 
       &quot;  'practice': 925,\n&quot;, 
       &quot;  'calls': 926,\n&quot;, 
       &quot;  '•': 927,\n&quot;, 
       &quot;  'UK': 928,\n&quot;, 
       &quot;  'force': 929,\n&quot;, 
       &quot;  'student': 930,\n&quot;, 
       &quot;  'idea': 931,\n&quot;, 
       &quot;  'reached': 932,\n&quot;, 
       &quot;  'reason': 933,\n&quot;, 
       &quot;  'levels': 934,\n&quot;, 
       &quot;  'space': 935,\n&quot;, 
       &quot;  'competition': 936,\n&quot;, 
       &quot;  'forces': 937,\n&quot;, 
       &quot;  'sector': 938,\n&quot;, 
       &quot;  'Last': 939,\n&quot;, 
       &quot;  'tried': 940,\n&quot;, 
       &quot;  'common': 941,\n&quot;, 
       &quot;  'homes': 942,\n&quot;, 
       &quot;  'stage': 943,\n&quot;, 
       &quot;  'department': 944,\n&quot;, 
       &quot;  'named': 945,\n&quot;, 
       &quot;  'earnings': 946,\n&quot;, 
       &quot;  'offers': 947,\n&quot;, 
       &quot;  'star': 948,\n&quot;, 
       &quot;  'certain': 949,\n&quot;, 
       &quot;  'double': 950,\n&quot;, 
       &quot;  'longer': 951,\n&quot;, 
       &quot;  'followed': 952,\n&quot;, 
       &quot;  'cause': 953,\n&quot;, 
       &quot;  'Association': 954,\n&quot;, 
       &quot;  'signed': 955,\n&quot;, 
       &quot;  'committee': 956,\n&quot;, 
       &quot;  'hour': 957,\n&quot;, 
       &quot;  'college': 958,\n&quot;, 
       &quot;  'Pakistan': 959,\n&quot;, 
       &quot;  'users': 960,\n&quot;, 
       &quot;  'Iran': 961,\n&quot;, 
       &quot;  'sign': 962,\n&quot;, 
       &quot;  'living': 963,\n&quot;, 
       &quot;  'failed': 964,\n&quot;, 
       &quot;  'reach': 965,\n&quot;, 
       &quot;  'quickly': 966,\n&quot;, 
       &quot;  'receive': 967,\n&quot;, 
       &quot;  'debt': 968,\n&quot;, 
       &quot;  'sale': 969,\n&quot;, 
       &quot;  'Board': 970,\n&quot;, 
       &quot;  'Americans': 971,\n&quot;, 
       &quot;  'Road': 972,\n&quot;, 
       &quot;  'Brown': 973,\n&quot;, 
       &quot;  'insurance': 974,\n&quot;, 
       &quot;  '##:##': 975,\n&quot;, 
       &quot;  'anyone': 976,\n&quot;, 
       &quot;  'tournament': 977,\n&quot;, 
       &quot;  'More': 978,\n&quot;, 
       &quot;  'gas': 979,\n&quot;, 
       &quot;  'talks': 980,\n&quot;, 
       &quot;  'serious': 981,\n&quot;, 
       &quot;  'required': 982,\n&quot;, 
       &quot;  'sell': 983,\n&quot;, 
       &quot;  'construction': 984,\n&quot;, 
       &quot;  'evidence': 985,\n&quot;, 
       &quot;  'remains': 986,\n&quot;, 
       &quot;  'black': 987,\n&quot;, 
       &quot;  'below': 988,\n&quot;, 
       &quot;  'improve': 989,\n&quot;, 
       &quot;  'crisis': 990,\n&quot;, 
       &quot;  'address': 991,\n&quot;, 
       &quot;  'questions': 992,\n&quot;, 
       &quot;  'easy': 993,\n&quot;, 
       &quot;  'begin': 994,\n&quot;, 
       &quot;  'view': 995,\n&quot;, 
       &quot;  'School': 996,\n&quot;, 
       &quot;  'heard': 997,\n&quot;, 
       &quot;  'executive': 998,\n&quot;, 
       &quot;  'raised': 999,\n&quot;, 
       &quot;  ...},\n&quot;, 
       &quot; tensor([[ 1.1292e-03, -8.9645e-04,  3.1853e-04,  ..., -1.5640e-03,\n&quot;, 
       &quot;          -1.2302e-04, -8.6308e-05],\n&quot;, 
       &quot;         [ 7.0312e-02,  8.6914e-02,  8.7891e-02,  ..., -4.7607e-02,\n&quot;, 
       &quot;           1.4465e-02, -6.2500e-02],\n&quot;, 
       &quot;         [-1.1780e-02, -4.7363e-02,  4.4678e-02,  ...,  7.1289e-02,\n&quot;, 
       &quot;          -3.4912e-02,  2.4170e-02],\n&quot;, 
       &quot;         ...,\n&quot;, 
       &quot;         [ 3.2715e-02, -3.2227e-02,  3.6133e-02,  ..., -8.8501e-03,\n&quot;, 
       &quot;           2.6978e-02,  1.9043e-02],\n&quot;, 
       &quot;         [ 4.5166e-02, -4.5166e-02, -3.9368e-03,  ...,  7.9590e-02,\n&quot;, 
       &quot;           7.2266e-02,  1.3000e-02],\n&quot;, 
       &quot;         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n&quot;, 
       &quot;           0.0000e+00,  0.0000e+00]], device='cuda:0'))&quot; 
      ] 
     }, 
     &quot;execution_count&quot;: 7, 
     &quot;metadata&quot;: {}, 
     &quot;output_type&quot;: &quot;execute_result&quot; 
    } 
   ], 
   &quot;source&quot;: [ 
    &quot;def parse_word2vec(word2vec_embeddings, embedding_components) -&gt; tuple[dict[str, int], torch.Tensor]:\n&quot;, 
    &quot;        word_labels: dict[str, int] = {}\n&quot;, 
    &quot;        tensor: torch.Tensor = torch.empty((EMBED_SIZE + 1, embedding_components), dtype=torch.float32, device=device)\n&quot;, 
    &quot;        \n&quot;, 
    &quot;        # Clean up the file and load the embeddings into a tensor\n&quot;, 
    &quot;        loop_idx = 0\n&quot;, 
    &quot;        for word, idx in word2vec_embeddings.key_to_index.items():\n&quot;, 
    &quot;            word_labels[word] = idx\n&quot;, 
    &quot;            tensor[idx] = torch.tensor(word2vec_embeddings.get_vector(word), dtype=torch.float32,\n&quot;, 
    &quot;                                         device=device)\n&quot;, 
    &quot;            # Output our progress every 100,000 words\n&quot;, 
    &quot;            if (loop_idx + 1) % 100000 == 0:\n&quot;, 
    &quot;                print(\&quot;Processed {}/{}\&quot;.format(loop_idx + 1, EMBED_SIZE))\n&quot;, 
    &quot;            loop_idx += 1\n&quot;, 
    &quot;        tensor[-1] = torch.zeros(embedding_components, dtype=torch.float32, device=device)\n&quot;, 
    &quot;\n&quot;, 
    &quot;        # Adding a padding token\n&quot;, 
    &quot;        word_labels[\&quot;&lt;PAD&gt;\&quot;] = EMBED_SIZE\n&quot;, 
    &quot;        tensor.to(device)\n&quot;, 
    &quot;        return word_labels, tensor\n&quot;, 
    &quot;\n&quot;, 
    &quot;\&quot;\&quot;\&quot;Deserialize the embeddings, and return word labels with their corresponding tensors.\&quot;\&quot;\&quot;\n&quot;, 
    &quot;def get_vectors(embedding: str) -&gt; tuple[dict[str, int], torch.Tensor]:\n&quot;, 
    &quot;    skip_first_line: bool = False\n&quot;, 
    &quot;    global EMBED_SIZE\n&quot;, 
    &quot;    match embedding:\n&quot;, 
    &quot;        case \&quot;glove\&quot;:\n&quot;, 
    &quot;            embedding_path: str = EMBEDDINGS_PATH + \&quot;glove.840B.300d.txt\&quot;\n&quot;, 
    &quot;            EMBED_SIZE = 2196018\n&quot;, 
    &quot;            embedding_components: int = 300\n&quot;, 
    &quot;        case \&quot;word2vec\&quot;:\n&quot;, 
    &quot;            embedding_path: str = EMBEDDINGS_PATH + \&quot;GoogleNews-vectors-negative300.bin\&quot;\n&quot;, 
    &quot;            gn_model = KeyedVectors.load_word2vec_format(embedding_path, binary=True)\n&quot;, 
    &quot;            embedding_components: int = 300\n&quot;, 
    &quot;            EMBED_SIZE = 3000000\n&quot;, 
    &quot;            return parse_word2vec(gn_model, embedding_components)\n&quot;, 
    &quot;        case \&quot;numberbatch\&quot;:\n&quot;, 
    &quot;            embedding_path: str = EMBEDDINGS_PATH + \&quot;numberbatch-19.08-en.txt\&quot;\n&quot;, 
    &quot;            EMBED_SIZE = 516782\n&quot;, 
    &quot;            embedding_components: int = 300\n&quot;, 
    &quot;            skip_first_line = True\n&quot;, 
    &quot;        case default:\n&quot;, 
    &quot;            raise RuntimeError(\&quot;Invalid embedding chosen.\&quot;)\n&quot;, 
    &quot;        \n&quot;, 
    &quot;    # Deserializing glove and numberbatch embeddings\n&quot;, 
    &quot;    if not os.path.exists(embedding_path):\n&quot;, 
    &quot;        raise FileNotFoundError(\&quot;Could not find embedding file: {}\&quot;.format(embedding_path))\n&quot;, 
    &quot;    with (open(embedding_path, encoding=\&quot;utf_8\&quot;) as embeddings_file):\n&quot;, 
    &quot;        word_labels: dict[str, int] = {}\n&quot;, 
    &quot;        tensor: torch.Tensor = torch.empty((EMBED_SIZE + 1, embedding_components), dtype=torch.float32, device=device)\n&quot;, 
    &quot;        \n&quot;, 
    &quot;        # We need to skip the first line of the numberbatch embeddings because that's header information\n&quot;, 
    &quot;        if skip_first_line:\n&quot;, 
    &quot;            _ = embeddings_file.readline()\n&quot;, 
    &quot;        \n&quot;, 
    &quot;        # Clean up the file and load the embeddings into a tensor\n&quot;, 
    &quot;        for index, embedding in enumerate(embeddings_file):\n&quot;, 
    &quot;            embedding_split: list[str] = embedding.rstrip().split(\&quot; \&quot;)\n&quot;, 
    &quot;            word_labels[embedding_split[0]] = index # Assign the word to the index\n&quot;, 
    &quot;            tensor[index] = torch.tensor([float(val) for val in embedding_split[1:]], dtype=torch.float32, # Every element except the first is converted to a float\n&quot;, 
    &quot;                                         device=device)\n&quot;, 
    &quot;            # Output our progress every 100,000 words\n&quot;, 
    &quot;            if (index + 1) % 100000 == 0:\n&quot;, 
    &quot;                print(\&quot;Processed {}/{}\&quot;.format(index + 1, EMBED_SIZE))\n&quot;, 
    &quot;        tensor[-1] = torch.zeros(embedding_components, dtype=torch.float32, device=device)\n&quot;, 
    &quot;\n&quot;, 
    &quot;        # Adding a padding token\n&quot;, 
    &quot;        word_labels[\&quot;&lt;PAD&gt;\&quot;] = EMBED_SIZE\n&quot;, 
    &quot;        tensor.to(device)\n&quot;, 
    &quot;        return word_labels, tensor&quot; 
   ] 
  }, 
  { 
   &quot;cell_type&quot;: &quot;code&quot;, 
   &quot;execution_count&quot;: null, 
   &quot;metadata&quot;: {}, 
   &quot;outputs&quot;: [], 
   &quot;source&quot;: [ 
    &quot;# Download link for word2vec: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n&quot;, 
    &quot;# Download link for Glove: https://huggingface.co/stanfordnlp/glove/resolve/main/glove.840B.300d.zip\n&quot;, 
    &quot;\n&quot;, 
    &quot;import matplotlib.pyplot as plt\n&quot;, 
    &quot;import numpy as np\n&quot;, 
    &quot;from numpy import ndarray\n&quot;, 
    &quot;from sklearn.manifold import TSNE\n&quot;, 
    &quot;from torch import Tensor, cat\n&quot;, 
    &quot;from torch.cuda import is_available as cuda_is_available\n&quot;, 
    &quot;from train import get_vectors\n&quot;, 
    &quot;\n&quot;, 
    &quot;\n&quot;, 
    &quot;# EDITABLE VARIABLES\n&quot;, 
    &quot;embeddings = [\&quot;numberbatch\&quot;, \&quot;glove\&quot;] # Embeddings definition. Add word2vec once deserialization is done.\n&quot;, 
    &quot;\n&quot;, 
    &quot;# Word comparison groups. Format: [base_word, similar_word_1, similar_word_2]\n&quot;, 
    &quot;word_comparison_groups = [\n&quot;, 
    &quot;    [\&quot;tire\&quot;, \&quot;tired\&quot;, \&quot;tyre\&quot;],\n&quot;, 
    &quot;    [\&quot;chest\&quot;, \&quot;thorax\&quot;, \&quot;crate\&quot;],\n&quot;, 
    &quot;    [\&quot;fall\&quot;, \&quot;autumn\&quot;, \&quot;plummet\&quot;],\n&quot;, 
    &quot;    [\&quot;bear\&quot;, \&quot;carry\&quot;, \&quot;ursidae\&quot;],\n&quot;, 
    &quot;    [\&quot;proof\&quot;, \&quot;evidence\&quot;, \&quot;testing\&quot;],\n&quot;, 
    &quot;    [\&quot;tear\&quot;, \&quot;rip\&quot;, \&quot;droplet\&quot;],\n&quot;, 
    &quot;    [\&quot;salty\&quot;, \&quot;flavored\&quot;, \&quot;tough\&quot;],\n&quot;, 
    &quot;    [\&quot;land\&quot;, \&quot;earth\&quot;, \&quot;arrive\&quot;],\n&quot;, 
    &quot;    [\&quot;close\&quot;, \&quot;near\&quot;, \&quot;shut\&quot;],\n&quot;, 
    &quot;    [\&quot;light\&quot;, \&quot;illumination\&quot;, \&quot;lightweight\&quot;],\n&quot;, 
    &quot;    [\&quot;match\&quot;, \&quot;competition\&quot;, \&quot;firestarter\&quot;],\n&quot;, 
    &quot;    [\&quot;wind\&quot;, \&quot;breeze\&quot;, \&quot;coil\&quot;],\n&quot;, 
    &quot;    [\&quot;fly\&quot;, \&quot;diptera\&quot;, \&quot;flight\&quot;],\n&quot;, 
    &quot;    [\&quot;lead\&quot;, \&quot;guide\&quot;, \&quot;pb\&quot;],\n&quot;, 
    &quot;    [\&quot;watch\&quot;, \&quot;observe\&quot;, \&quot;wristwatch\&quot;],\n&quot;, 
    &quot;    [\&quot;file\&quot;, \&quot;organize\&quot;, \&quot;rasp\&quot;],\n&quot;, 
    &quot;    [\&quot;change\&quot;, \&quot;modify\&quot;, \&quot;currency\&quot;],\n&quot;, 
    &quot;    [\&quot;bark\&quot;, \&quot;woof\&quot;, \&quot;skin\&quot;],\n&quot;, 
    &quot;    [\&quot;refuse\&quot;, \&quot;decline\&quot;, \&quot;garbage\&quot;],\n&quot;, 
    &quot;    [\&quot;protest\&quot;, \&quot;rally\&quot;, \&quot;complain\&quot;],\n&quot;, 
    &quot;    [\&quot;escort\&quot;, \&quot;companion\&quot;, \&quot;accompany\&quot;],\n&quot;, 
    &quot;    [\&quot;produce\&quot;, \&quot;food\&quot;, \&quot;make\&quot;],\n&quot;, 
    &quot;    [\&quot;desert\&quot;, \&quot;abandon\&quot;, \&quot;wilderness\&quot;],\n&quot;, 
    &quot;    [\&quot;live\&quot;, \&quot;stream\&quot;, \&quot;exist\&quot;],\n&quot;, 
    &quot;    [\&quot;house\&quot;, \&quot;home\&quot;, \&quot;contain\&quot;]\n&quot;, 
    &quot;]\n&quot;, 
    &quot;\n&quot;, 
    &quot;# Add a new distance function here if you want.\n&quot;, 
    &quot;\&quot;\&quot;\&quot;Calculate the distances between a base word and two similar words using Euclidean, Cosine, and Manhattan distances.\&quot;\&quot;\&quot;\n&quot;, 
    &quot;def calculate_distances(base_word: ndarray[float], similar_word_1: ndarray[float], similar_word_2: ndarray[float]) -&gt; dict[str, list[float]]:\n&quot;, 
    &quot;    return {\n&quot;, 
    &quot;        \&quot;euclidean\&quot;: [euclidean_distance(similar_word_1, base_word), euclidean_distance(similar_word_2, base_word)],\n&quot;, 
    &quot;        \&quot;cosine\&quot;: [cosine_similarity(similar_word_1, base_word), cosine_similarity(similar_word_2, base_word)],\n&quot;, 
    &quot;        \&quot;manhattan\&quot;: [manhattan_distance(similar_word_1, base_word), manhattan_distance(similar_word_2, base_word)],\n&quot;, 
    &quot;    }\n&quot;, 
    &quot;\n&quot;, 
    &quot;# We should look at comparing vectors in different embeddings and see how well ambigious words center around common\n&quot;, 
    &quot;# synonyms for each meaning. We could probably do some sort of visualization for this as well.\n&quot;, 
    &quot;\n&quot;, 
    &quot;def euclidean_distance(vector1: Tensor, vector2: Tensor) -&gt; float:\n&quot;, 
    &quot;    return np.linalg.norm(vector1 - vector2)\n&quot;, 
    &quot;\n&quot;, 
    &quot;def cosine_similarity(vector1: Tensor, vector2: Tensor) -&gt; float:\n&quot;, 
    &quot;    return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n&quot;, 
    &quot;\n&quot;, 
    &quot;def manhattan_distance(v1, v2):\n&quot;, 
    &quot;    return np.sum(np.abs(v1 - v2))&quot; 
   ] 
  }, 
  { 
   &quot;cell_type&quot;: &quot;code&quot;, 
   &quot;execution_count&quot;: null, 
   &quot;metadata&quot;: {}, 
   &quot;outputs&quot;: [], 
   &quot;source&quot;: [ 
    &quot;\&quot;\&quot;\&quot;Converts the given list of tensors to a numpy array based on GPU availability.\&quot;\&quot;\&quot;\n&quot;, 
    &quot;def convert_tensors_to_numpy(embeddings_list: list[Tensor]) -&gt; ndarray[float]:\n&quot;, 
    &quot;    if cuda_is_available():\n&quot;, 
    &quot;        numpy_vectors = np.array([vector.cpu().numpy() for vector in embeddings_list])\n&quot;, 
    &quot;    else:\n&quot;, 
    &quot;        numpy_vectors = np.array([vector.numpy() for vector in embeddings_list])\n&quot;, 
    &quot;    return numpy_vectors\n&quot;, 
    &quot;\n&quot;, 
    &quot;\&quot;\&quot;\&quot;Returns the word vectors to compare from the given numpy vectors. These vectors are a word groups members\&quot;\&quot;\&quot;\n&quot;, 
    &quot;def get_word_vectors_to_compare(numpy_vectors: ndarray[float]) -&gt; tuple[ndarray[float], ndarray[float], ndarray[float]]:\n&quot;, 
    &quot;    return numpy_vectors[0], numpy_vectors[1], numpy_vectors[2]\n&quot;, 
    &quot;\n&quot;, 
    &quot;\&quot;\&quot;\&quot;Returns a dictionary of distance types to their calculated distances for the given word vectors.\&quot;\&quot;\&quot;\n&quot;, 
    &quot;def get_distances(embeddings_list: list[Tensor]) -&gt; dict[str, list[float]]:\n&quot;, 
    &quot;    numpy_vectors = convert_tensors_to_numpy(embeddings_list)\n&quot;, 
    &quot;    base_word, similar_word1, similar_word2 = get_word_vectors_to_compare(numpy_vectors)\n&quot;, 
    &quot;    return calculate_distances(base_word, similar_word1, similar_word2)\n&quot;, 
    &quot;\n&quot;, 
    &quot;\&quot;\&quot;\&quot;Returns a dictionary of embedding names to their respective word vectors and vocabularies\&quot;\&quot;\&quot;\n&quot;, 
    &quot;def get_embeddings(embeddings: list[str]) -&gt; dict[str, (dict[str, int], Tensor)]:\n&quot;, 
    &quot;    if len(embeddings) == 0:\n&quot;, 
    &quot;        raise ValueError(\&quot;No embeddings were selected to load.\&quot;)\n&quot;, 
    &quot;    \n&quot;, 
    &quot;    result = {}\n&quot;, 
    &quot;    for embedding in embeddings:\n&quot;, 
    &quot;        vocab, vectors = get_vectors(embedding)\n&quot;, 
    &quot;        result[embedding] = (vocab, vectors)\n&quot;, 
    &quot;    return result\n&quot;, 
    &quot;\n&quot;, 
    &quot;\&quot;\&quot;\&quot;Returns a dictionary of embedding names to a list of comparison groups, the words whose distances are being compared\&quot;\&quot;\&quot;\n&quot;, 
    &quot;def get_comparison_embeddings(embeddings: dict[str, (dict[str, int], Tensor)]) -&gt; dict[str, list[list[Tensor]]]:\n&quot;, 
    &quot;    # Dictionary of embedding name to list of comparison groups\n&quot;, 
    &quot;    result: dict[str, list[list[Tensor]]] = {}\n&quot;, 
    &quot;    for embed_name, (vocab, vectors) in embeddings.items():\n&quot;, 
    &quot;        comparisons = []\n&quot;, 
    &quot;        if embed_name not in result:\n&quot;, 
    &quot;            result[embed_name] = []\n&quot;, 
    &quot;        \n&quot;, 
    &quot;        # Populate the comparison groups\n&quot;, 
    &quot;        for idx, group in enumerate(word_comparison_groups):\n&quot;, 
    &quot;            comparisons = []\n&quot;, 
    &quot;            for word in group:\n&quot;, 
    &quot;                comparisons.append(vectors[vocab[word]])\n&quot;, 
    &quot;            result[embed_name].append(comparisons)\n&quot;, 
    &quot;    return result\n&quot;, 
    &quot;\n&quot;, 
    &quot;\&quot;\&quot;\&quot;Returns a dictionary of embedding names to a list of dictionaries of distance types to their calculated distances\&quot;\&quot;\&quot;\n&quot;, 
    &quot;def compare_embeddings(comparison_embeddings: dict[str, list[list[Tensor]]]) -&gt; dict[str, list[dict[str, list[float]]]]:\n&quot;, 
    &quot;    result = {}\n&quot;, 
    &quot;    for embedding, word_groups in comparison_embeddings.items():\n&quot;, 
    &quot;        for idx, group_vectors in enumerate(word_groups):\n&quot;, 
    &quot;            if result.get(embedding) is None:\n&quot;, 
    &quot;                result[embedding] = []\n&quot;, 
    &quot;            distances: dict[str, list[float]] = get_distances(group_vectors)\n&quot;, 
    &quot;            result[embedding].append(distances)\n&quot;, 
    &quot;    return result&quot; 
   ] 
  }, 
  { 
   &quot;cell_type&quot;: &quot;code&quot;, 
   &quot;execution_count&quot;: null, 
   &quot;metadata&quot;: {}, 
   &quot;outputs&quot;: [], 
   &quot;source&quot;: [ 
    &quot;\n&quot;, 
    &quot;\&quot;\&quot;\&quot;Plots boxplots of the distance ratios for each embedding and distance type.\&quot;\&quot;\&quot;\n&quot;, 
    &quot;def compare_distances(embedding_distances: dict[str, list[dict[str, list[float]]]]):\n&quot;, 
    &quot;    \&quot;\&quot;\&quot;Returns a dictionary of embedding names to a dictionary of distance types to their calculated distance ratios.\&quot;\&quot;\&quot;\n&quot;, 
    &quot;    def calculate_distance_ratios():\n&quot;, 
    &quot;        # Dict of embedding name to distance type and all of that distance type's calculated distance ratios\n&quot;, 
    &quot;        ratios: dict[str, dict[str, list[float]]] = {}\n&quot;, 
    &quot;        # For every distance type for embeddings\n&quot;, 
    &quot;        for embedding_name, distances in embedding_distances.items():\n&quot;, 
    &quot;            ratios[embedding_name] = {}\n&quot;, 
    &quot;            for distance in distances:\n&quot;, 
    &quot;                for distance_type, values in distance.items():\n&quot;, 
    &quot;                    if ratios[embedding_name].get(distance_type) is None:\n&quot;, 
    &quot;                        ratios[embedding_name][distance_type] = []\n&quot;, 
    &quot;                    ratio = (max(values[0], values[1]) / min(values[0], values[1]))\n&quot;, 
    &quot;                    ratios[embedding_name][distance_type].append(ratio)\n&quot;, 
    &quot;        return ratios\n&quot;, 
    &quot;\n&quot;, 
    &quot;    \&quot;\&quot;\&quot;\n&quot;, 
    &quot;    Plots boxplots of the distance ratios for each embedding and distance type.\n&quot;, 
    &quot;    Args: ratios: dict[str, dict[str, list[float]]] - Dictionary of embedding names to a dictionary of distance types to a list of all their calculated distance ratios.\n&quot;, 
    &quot;    \n&quot;, 
    &quot;    Ex:\n&quot;, 
    &quot;    ratios = {\n&quot;, 
    &quot;        \&quot;glove\&quot;: {\n&quot;, 
    &quot;            \&quot;euclidean\&quot;: [1.0, 2.55, 1.380, 4.44, 4.4, 4983],\n&quot;, 
    &quot;            \&quot;cosine\&quot;: [1.0, 2.55, 1.380, 4.44, 4.4, 4983],\n&quot;, 
    &quot;            ...\n&quot;, 
    &quot;        },\n&quot;, 
    &quot;        \&quot;numberbatch\&quot;: {\n&quot;, 
    &quot;            \&quot;euclidean\&quot;: [1.0, 2.55, 1.380, 4.44, 4.4, 4983],\n&quot;, 
    &quot;            \&quot;cosine\&quot;: [1.0, 2.55, 1.380, 4.44, 4.4, 4983],\n&quot;, 
    &quot;            ...        \n&quot;, 
    &quot;        }\n&quot;, 
    &quot;    }\n&quot;, 
    &quot;    \&quot;\&quot;\&quot;\n&quot;, 
    &quot;    def show_boxplots(ratios: dict[str, dict[str, list[float]]]):\n&quot;, 
    &quot;        distances: list[dict[str, list[float]]] = list(ratios.values())\n&quot;, 
    &quot;        # Get all the values per distance type for every embedding\n&quot;, 
    &quot;        to_plot = {}\n&quot;, 
    &quot;        for i in range(0, len(distances)):\n&quot;, 
    &quot;            for embedding_name, distances2 in ratios.items():\n&quot;, 
    &quot;                for distance_type, values in distances2.items():\n&quot;, 
    &quot;                    print(embedding_name, distance_type, values)\n&quot;, 
    &quot;                    if to_plot.get(distance_type) is None:\n&quot;, 
    &quot;                        to_plot[distance_type] = {}\n&quot;, 
    &quot;                    to_plot[distance_type][embedding_name] = values\n&quot;, 
    &quot;        \n&quot;, 
    &quot;        # For every distance metric, plot boxplots for all embeddings\n&quot;, 
    &quot;        for distance_type, embedding_data in to_plot.items():\n&quot;, 
    &quot;            fig, axs = plt.subplots(figsize=(10, 8))\n&quot;, 
    &quot;            boxplots_data = []\n&quot;, 
    &quot;            labels = []\n&quot;, 
    &quot;            for embedding_name, values in embedding_data.items():\n&quot;, 
    &quot;                boxplots_data.append(values)\n&quot;, 
    &quot;                labels.append(embedding_name)\n&quot;, 
    &quot;            axs.boxplot(boxplots_data)\n&quot;, 
    &quot;            axs.set_xticklabels(labels)\n&quot;, 
    &quot;            axs.set_title(distance_type)\n&quot;, 
    &quot;            plt.tight_layout()\n&quot;, 
    &quot;            plt.show()\n&quot;, 
    &quot;    \n&quot;, 
    &quot;    # Dict of embedding name to distance type and its calculated distance ratio\n&quot;, 
    &quot;    ratios = calculate_distance_ratios()\n&quot;, 
    &quot;    show_boxplots(ratios)&quot; 
   ] 
  }, 
  { 
   &quot;cell_type&quot;: &quot;code&quot;, 
   &quot;execution_count&quot;: null, 
   &quot;metadata&quot;: {}, 
   &quot;outputs&quot;: [], 
   &quot;source&quot;: [ 
    &quot;orig_embeddings = get_embeddings(embeddings)\n&quot;, 
    &quot;comp_embeddings = get_comparison_embeddings(orig_embeddings)\n&quot;, 
    &quot;comp_distances = compare_embeddings(comp_embeddings)\n&quot;, 
    &quot;compare_distances(comp_distances)&quot; 
   ] 
  }, 
  { 
   &quot;cell_type&quot;: &quot;markdown&quot;, 
   &quot;source&quot;: [ 
    &quot;# Analysis findings\n&quot;, 
    &quot;Let us consider the following figures/graphs:\n&quot;, 
    &quot;   ![](cosine.png)\n&quot;, 
    &quot;   ![](euclidean.png)\n&quot;, 
    &quot;   ![](manhattan.png)\n&quot;, 
    &quot;As we can see, the figures show a result that almost seems too close to call. In all cases, Glove &amp; Numberbatch have distance ratios within about ~0.1, with the cosine similarity having a wider margin, with a median difference of about a full point between Numberbatch &amp; Glove.\n&quot;, 
    &quot;While these margins may certainly seem slim, the consistency of the results does present a tendency towards an answer to our original research questions. Rather than seeing an improved performance from Numberbatch in relation to how well it averages ambiguous word definitions, it instead performance consistently worse, tending to have a worse bias towards one definition over another in the average cases across all tested metrics. This does not necessarily indicate an absolute performance metric being appreciatively worse, but the IQR being significantly smaller in all 3 metrics does indicate that there may be an overall tendency for Numberbatch to center on singular definitions of words (or at least some subset of definitions) over evenly representing all ambiguous word senses within a given embedding.\n&quot;, 
    &quot;As such, we can confidently reject our hypothesis: Numberbatch’s efforts to debias &amp; its multi-lingual capabilities have not represented in any improved performance in relation to the embeddings’ ability to evenly represent word definitions. Rather, Numberbatch’s performance is consistently worse across all measured metrics, with its IQRs and median tending to result in a larger ratio (worse) on average versus Glove’s results. Therefore, we state that our hypothesis was incorrect and Numberbatch does not have superior performance vis-à-vis having semantic vectors that are near-equidistant to disambiguated definitional senses.\n&quot; 
   ], 
   &quot;metadata&quot;: { 
    &quot;collapsed&quot;: false 
   } 
  }, 
  { 
   &quot;cell_type&quot;: &quot;markdown&quot;, 
   &quot;source&quot;: [ 
    &quot;# Potential conferences or journals\n&quot;, 
    &quot;We believe that the results of our research, if sufficiently beefed up to introduce additional rigor and analysis, may \n&quot;, 
    &quot;be of interests to a conference such as The International Conference on Learning Representations (ICLR), which has already hosted papers on bias mitigation &amp; analysis, as well as The Conference on Fairness, Accountability, and Transparency (FAccT), which has research on the ethical considerations and transparency of AI as one of its focal points. We believe that our work representations a meaningful avenue of research within the considerations of both bias and transparency within Ais, and believe these conferences would be well suited to the publishment of our work.\n&quot; 
   ], 
   &quot;metadata&quot;: { 
    &quot;collapsed&quot;: false 
   } 
  }, 
  { 
   &quot;cell_type&quot;: &quot;markdown&quot;, 
   &quot;source&quot;: [], 
   &quot;metadata&quot;: { 
    &quot;collapsed&quot;: false 
   } 
  } 
 ], 
 &quot;metadata&quot;: { 
  &quot;kernelspec&quot;: { 
   &quot;display_name&quot;: &quot;remote-venv&quot;, 
   &quot;language&quot;: &quot;python&quot;, 
   &quot;name&quot;: &quot;python3&quot; 
  }, 
  &quot;language_info&quot;: { 
   &quot;codemirror_mode&quot;: { 
    &quot;name&quot;: &quot;ipython&quot;, 
    &quot;version&quot;: 3 
   }, 
   &quot;file_extension&quot;: &quot;.py&quot;, 
   &quot;mimetype&quot;: &quot;text/x-python&quot;, 
   &quot;name&quot;: &quot;python&quot;, 
   &quot;nbconvert_exporter&quot;: &quot;python&quot;, 
   &quot;pygments_lexer&quot;: &quot;ipython3&quot;, 
   &quot;version&quot;: &quot;3.10.13&quot; 
  } 
 }, 
 &quot;nbformat&quot;: 4, 
 &quot;nbformat_minor&quot;: 2 
} 
</span></pre>
</body>
</html>